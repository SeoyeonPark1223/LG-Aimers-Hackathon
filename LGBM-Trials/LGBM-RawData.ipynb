{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10634121,"sourceType":"datasetVersion","datasetId":6583998}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# LGBM - 전체 데이터셋","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-11T11:48:55.068343Z","iopub.execute_input":"2025-02-11T11:48:55.068794Z","iopub.status.idle":"2025-02-11T11:48:55.077074Z","shell.execute_reply.started":"2025-02-11T11:48:55.068755Z","shell.execute_reply":"2025-02-11T11:48:55.076271Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/lg-aimers/sample_submission.csv\n/kaggle/input/lg-aimers/train.csv\n/kaggle/input/lg-aimers/test.csv\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"data_path = '/kaggle/input/lg-aimers/'\n\ntrain = pd.read_csv(data_path + 'train.csv', index_col= 'ID')\ntest = pd.read_csv(data_path + 'test.csv', index_col= 'ID')\nsubmission = pd.read_csv(data_path + 'sample_submission.csv', index_col= 'ID')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T11:48:55.078213Z","iopub.execute_input":"2025-02-11T11:48:55.078500Z","iopub.status.idle":"2025-02-11T11:48:57.496813Z","shell.execute_reply.started":"2025-02-11T11:48:55.078471Z","shell.execute_reply":"2025-02-11T11:48:57.496028Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"## 1. 데이터 통합","metadata":{}},{"cell_type":"code","source":"all_data = pd.concat([train, test], ignore_index= True)\nall_data = all_data.drop('임신 성공 여부', axis= 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T11:48:57.498476Z","iopub.execute_input":"2025-02-11T11:48:57.498789Z","iopub.status.idle":"2025-02-11T11:48:57.807871Z","shell.execute_reply.started":"2025-02-11T11:48:57.498758Z","shell.execute_reply":"2025-02-11T11:48:57.807180Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"categorical_columns = [\n    \"시술 시기 코드\",\n    \"시술 당시 나이\",\n    \"시술 유형\",\n    \"특정 시술 유형\",\n    \"배란 자극 여부\",\n    \"배란 유도 유형\",\n    \"단일 배아 이식 여부\",\n    \"착상 전 유전 검사 사용 여부\",\n    \"착상 전 유전 진단 사용 여부\",\n    \"남성 주 불임 원인\",\n    \"남성 부 불임 원인\",\n    \"여성 주 불임 원인\",\n    \"여성 부 불임 원인\",\n    \"부부 주 불임 원인\",\n    \"부부 부 불임 원인\",\n    \"불명확 불임 원인\",\n    \"불임 원인 - 난관 질환\",\n    \"불임 원인 - 남성 요인\",\n    \"불임 원인 - 배란 장애\",\n    \"불임 원인 - 여성 요인\",\n    \"불임 원인 - 자궁경부 문제\",\n    \"불임 원인 - 자궁내막증\",\n    \"불임 원인 - 정자 농도\",\n    \"불임 원인 - 정자 면역학적 요인\",\n    \"불임 원인 - 정자 운동성\",\n    \"불임 원인 - 정자 형태\",\n    \"배아 생성 주요 이유\",\n    \"총 시술 횟수\",\n    \"클리닉 내 총 시술 횟수\",\n    \"IVF 시술 횟수\",\n    \"DI 시술 횟수\",\n    \"총 임신 횟수\",\n    \"IVF 임신 횟수\",\n    \"DI 임신 횟수\",\n    \"총 출산 횟수\",\n    \"IVF 출산 횟수\",\n    \"DI 출산 횟수\",\n    \"난자 출처\",\n    \"정자 출처\",\n    \"난자 기증자 나이\",\n    \"정자 기증자 나이\",\n    \"동결 배아 사용 여부\",\n    \"신선 배아 사용 여부\",\n    \"기증 배아 사용 여부\",\n    \"대리모 여부\",\n    \"PGD 시술 여부\",\n    \"PGS 시술 여부\"\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T11:48:57.808927Z","iopub.execute_input":"2025-02-11T11:48:57.809176Z","iopub.status.idle":"2025-02-11T11:48:57.814121Z","shell.execute_reply.started":"2025-02-11T11:48:57.809155Z","shell.execute_reply":"2025-02-11T11:48:57.813279Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# 카테고리형 컬럼들을 문자열로 변환\nfor col in categorical_columns:\n    all_data[col] = all_data[col].astype(str)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T11:48:57.815025Z","iopub.execute_input":"2025-02-11T11:48:57.815361Z","iopub.status.idle":"2025-02-11T11:49:00.487844Z","shell.execute_reply.started":"2025-02-11T11:48:57.815325Z","shell.execute_reply":"2025-02-11T11:49:00.486880Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"## 2. 피처 엔지니어링: 원 핫 인코딩","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\nonehot_encoder = OneHotEncoder()\n\nencoded_cat_matrix = onehot_encoder.fit_transform(all_data[categorical_columns])\n\nencoded_cat_matrix","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T11:49:00.488820Z","iopub.execute_input":"2025-02-11T11:49:00.489153Z","iopub.status.idle":"2025-02-11T11:49:04.585732Z","shell.execute_reply.started":"2025-02-11T11:49:00.489119Z","shell.execute_reply":"2025-02-11T11:49:04.584818Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"<346418x205 sparse matrix of type '<class 'numpy.float64'>'\n\twith 16281646 stored elements in Compressed Sparse Row format>"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"all_columns = all_data.columns\nall_columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T11:49:04.586858Z","iopub.execute_input":"2025-02-11T11:49:04.587218Z","iopub.status.idle":"2025-02-11T11:49:04.592382Z","shell.execute_reply.started":"2025-02-11T11:49:04.587187Z","shell.execute_reply":"2025-02-11T11:49:04.591650Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"Index(['시술 시기 코드', '시술 당시 나이', '임신 시도 또는 마지막 임신 경과 연수', '시술 유형', '특정 시술 유형',\n       '배란 자극 여부', '배란 유도 유형', '단일 배아 이식 여부', '착상 전 유전 검사 사용 여부',\n       '착상 전 유전 진단 사용 여부', '남성 주 불임 원인', '남성 부 불임 원인', '여성 주 불임 원인',\n       '여성 부 불임 원인', '부부 주 불임 원인', '부부 부 불임 원인', '불명확 불임 원인', '불임 원인 - 난관 질환',\n       '불임 원인 - 남성 요인', '불임 원인 - 배란 장애', '불임 원인 - 여성 요인', '불임 원인 - 자궁경부 문제',\n       '불임 원인 - 자궁내막증', '불임 원인 - 정자 농도', '불임 원인 - 정자 면역학적 요인',\n       '불임 원인 - 정자 운동성', '불임 원인 - 정자 형태', '배아 생성 주요 이유', '총 시술 횟수',\n       '클리닉 내 총 시술 횟수', 'IVF 시술 횟수', 'DI 시술 횟수', '총 임신 횟수', 'IVF 임신 횟수',\n       'DI 임신 횟수', '총 출산 횟수', 'IVF 출산 횟수', 'DI 출산 횟수', '총 생성 배아 수',\n       '미세주입된 난자 수', '미세주입에서 생성된 배아 수', '이식된 배아 수', '미세주입 배아 이식 수', '저장된 배아 수',\n       '미세주입 후 저장된 배아 수', '해동된 배아 수', '해동 난자 수', '수집된 신선 난자 수', '저장된 신선 난자 수',\n       '혼합된 난자 수', '파트너 정자와 혼합된 난자 수', '기증자 정자와 혼합된 난자 수', '난자 출처', '정자 출처',\n       '난자 기증자 나이', '정자 기증자 나이', '동결 배아 사용 여부', '신선 배아 사용 여부', '기증 배아 사용 여부',\n       '대리모 여부', 'PGD 시술 여부', 'PGS 시술 여부', '난자 채취 경과일', '난자 해동 경과일',\n       '난자 혼합 경과일', '배아 이식 경과일', '배아 해동 경과일'],\n      dtype='object')"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"remaining_columns = list(set(all_columns) - set(categorical_columns))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T11:49:04.594452Z","iopub.execute_input":"2025-02-11T11:49:04.594647Z","iopub.status.idle":"2025-02-11T11:49:04.607111Z","shell.execute_reply.started":"2025-02-11T11:49:04.594630Z","shell.execute_reply":"2025-02-11T11:49:04.606325Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"from scipy import sparse\n\nextracted_data_sprs = sparse.hstack([sparse.csr_matrix(all_data[remaining_columns]),\n                               encoded_cat_matrix],\n                              format='csr')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T11:49:04.608367Z","iopub.execute_input":"2025-02-11T11:49:04.608593Z","iopub.status.idle":"2025-02-11T11:49:05.077871Z","shell.execute_reply.started":"2025-02-11T11:49:04.608564Z","shell.execute_reply":"2025-02-11T11:49:05.077182Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"## 3. 데이터 나누기","metadata":{}},{"cell_type":"code","source":"num_train = len(train) \n\nX = extracted_data_sprs[:num_train]\nX_test = extracted_data_sprs[num_train:]\n\ny = train['임신 성공 여부'].values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T11:49:05.078733Z","iopub.execute_input":"2025-02-11T11:49:05.078951Z","iopub.status.idle":"2025-02-11T11:49:05.328691Z","shell.execute_reply.started":"2025-02-11T11:49:05.078933Z","shell.execute_reply":"2025-02-11T11:49:05.327988Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"## 4. 모델 훈련","metadata":{}},{"cell_type":"markdown","source":"### 4-1. 층화 K 폴드","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\n\n# 층화 K 폴드 교차 검증기\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1991)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T11:49:05.329451Z","iopub.execute_input":"2025-02-11T11:49:05.329659Z","iopub.status.idle":"2025-02-11T11:49:05.333476Z","shell.execute_reply.started":"2025-02-11T11:49:05.329640Z","shell.execute_reply":"2025-02-11T11:49:05.332667Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"### 4-2. 베이지안 최적화","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\n\n# 8:2 비율로 훈련 데이터, 검증 데이터 분리 (베이지안 최적화 수행용)\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, \n                                                      test_size=0.2, \n                                                      random_state=0)\n\n# 베이지안 최적화용 데이터셋\nbayes_dtrain = lgb.Dataset(X_train, y_train)\nbayes_dvalid = lgb.Dataset(X_valid, y_valid)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T11:49:05.334353Z","iopub.execute_input":"2025-02-11T11:49:05.334650Z","iopub.status.idle":"2025-02-11T11:49:05.424327Z","shell.execute_reply.started":"2025-02-11T11:49:05.334628Z","shell.execute_reply":"2025-02-11T11:49:05.423614Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# 베이지안 최적화를 위한 하이퍼파라미터 범위\n# TODO: 점점 줄여나가자!!!!\nparam_bounds = {\n    'num_leaves': (20, 100),  \n    'lambda_l1': (0, 2),  \n    'lambda_l2': (0, 2),  \n    'feature_fraction': (0.5, 1),  \n    'bagging_fraction': (0.5, 1),  \n    'min_child_samples': (5, 50),  \n    'min_child_weight': (1, 50)  \n}\n\n# 값이 고정된 하이퍼파라미터\nfixed_params = {'objective': 'binary', # binary classification\n                'learning_rate': 0.005, # 0.01~0.001\n                'bagging_freq': 1, # 0 or 1\n                'force_row_wise': True,\n                'random_state': 1991}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T11:49:05.425193Z","iopub.execute_input":"2025-02-11T11:49:05.425457Z","iopub.status.idle":"2025-02-11T11:49:05.429990Z","shell.execute_reply.started":"2025-02-11T11:49:05.425435Z","shell.execute_reply":"2025-02-11T11:49:05.429160Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nfrom lightgbm import early_stopping\n\ndef eval_function(num_leaves, lambda_l1, lambda_l2, feature_fraction,\n                  bagging_fraction, min_child_samples, min_child_weight):\n    '''최적화하려는 평가지표 계산 함수'''\n    \n    # 베이지안 최적화를 수행할 하이퍼파라미터 \n    params = {'num_leaves': int(round(num_leaves)),\n              'lambda_l1': lambda_l1,\n              'lambda_l2': lambda_l2,\n              'feature_fraction': feature_fraction,\n              'bagging_fraction': bagging_fraction,\n              'min_child_samples': int(round(min_child_samples)),\n              'min_child_weight': min_child_weight,\n              'feature_pre_filter': False}\n    # 고정된 하이퍼파라미터도 추가\n    params.update(fixed_params)\n    \n    print('하이퍼파라미터:', params)    \n    \n    # LightGBM 모델 훈련\n    lgb_model = lgb.train(params=params, \n                           train_set=bayes_dtrain,\n                           num_boost_round=2500,\n                           valid_sets=bayes_dvalid,\n                           callbacks=[early_stopping(stopping_rounds=100)])\n    # 검증 데이터로 예측 수행\n    preds = lgb_model.predict(X_valid) \n    # roc-auc 계산\n    roc_auc = roc_auc_score(y_valid, preds)\n    print(f'roc-auc : {roc_auc}\\n')\n    \n    return roc_auc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T11:49:05.430884Z","iopub.execute_input":"2025-02-11T11:49:05.431203Z","iopub.status.idle":"2025-02-11T11:49:05.445857Z","shell.execute_reply.started":"2025-02-11T11:49:05.431174Z","shell.execute_reply":"2025-02-11T11:49:05.445047Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"from bayes_opt import BayesianOptimization\n\n# 베이지안 최적화 객체 생성\noptimizer = BayesianOptimization(f=eval_function,      # 평가지표 계산 함수\n                                 pbounds=param_bounds, # 하이퍼파라미터 범위\n                                 random_state=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T11:49:05.446654Z","iopub.execute_input":"2025-02-11T11:49:05.446907Z","iopub.status.idle":"2025-02-11T11:49:05.463110Z","shell.execute_reply.started":"2025-02-11T11:49:05.446876Z","shell.execute_reply":"2025-02-11T11:49:05.462510Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# 베이지안 최적화 수행\n# TODO: init_points 10~15, n_iter 30~70\noptimizer.maximize(init_points=10, n_iter=50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T11:49:05.463901Z","iopub.execute_input":"2025-02-11T11:49:05.464118Z","iopub.status.idle":"2025-02-11T12:29:43.896294Z","shell.execute_reply.started":"2025-02-11T11:49:05.464076Z","shell.execute_reply":"2025-02-11T12:29:43.895403Z"}},"outputs":[{"name":"stdout","text":"|   iter    |  target   | baggin... | featur... | lambda_l1 | lambda_l2 | min_ch... | min_ch... | num_le... |\n-------------------------------------------------------------------------------------------------------------\n하이퍼파라미터: {'num_leaves': 55, 'lambda_l1': 1.2055267521432877, 'lambda_l2': 1.0897663659937937, 'feature_fraction': 0.8575946831862098, 'bagging_fraction': 0.7744067519636624, 'min_child_samples': 24, 'min_child_weight': 32.64881154026615, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[1834]\tvalid_0's binary_logloss: 0.488864\nroc-auc : 0.7419709052936998\n\n| \u001b[39m1        \u001b[39m | \u001b[39m0.742    \u001b[39m | \u001b[39m0.7744   \u001b[39m | \u001b[39m0.8576   \u001b[39m | \u001b[39m1.206    \u001b[39m | \u001b[39m1.09     \u001b[39m | \u001b[39m24.06    \u001b[39m | \u001b[39m32.65    \u001b[39m | \u001b[39m55.01    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 94, 'lambda_l1': 0.7668830376515554, 'lambda_l2': 1.5834500761653292, 'feature_fraction': 0.9818313802505146, 'bagging_fraction': 0.9458865003910399, 'min_child_samples': 29, 'min_child_weight': 28.834183493602684, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[1360]\tvalid_0's binary_logloss: 0.489104\nroc-auc : 0.7414978872019141\n\n| \u001b[39m2        \u001b[39m | \u001b[39m0.7415   \u001b[39m | \u001b[39m0.9459   \u001b[39m | \u001b[39m0.9818   \u001b[39m | \u001b[39m0.7669   \u001b[39m | \u001b[39m1.583    \u001b[39m | \u001b[39m28.8     \u001b[39m | \u001b[39m28.83    \u001b[39m | \u001b[39m94.05    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 98, 'lambda_l1': 0.04043679488065144, 'lambda_l2': 1.665239691095876, 'feature_fraction': 0.5435646498507704, 'bagging_fraction': 0.5355180290989434, 'min_child_samples': 40, 'min_child_weight': 43.63059526409414, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[1495]\tvalid_0's binary_logloss: 0.489111\nroc-auc : 0.7417626303289311\n\n| \u001b[39m3        \u001b[39m | \u001b[39m0.7418   \u001b[39m | \u001b[39m0.5355   \u001b[39m | \u001b[39m0.5436   \u001b[39m | \u001b[39m0.04044  \u001b[39m | \u001b[39m1.665    \u001b[39m | \u001b[39m40.02    \u001b[39m | \u001b[39m43.63    \u001b[39m | \u001b[39m98.29    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 96, 'lambda_l1': 1.561058352572911, 'lambda_l2': 0.23654885173786644, 'feature_fraction': 0.7307396811264659, 'bagging_fraction': 0.8995792821083618, 'min_child_samples': 34, 'min_child_weight': 8.024311083043273, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[1390]\tvalid_0's binary_logloss: 0.489076\nroc-auc : 0.7415588280801115\n\n| \u001b[39m4        \u001b[39m | \u001b[39m0.7416   \u001b[39m | \u001b[39m0.8996   \u001b[39m | \u001b[39m0.7307   \u001b[39m | \u001b[39m1.561    \u001b[39m | \u001b[39m0.2365   \u001b[39m | \u001b[39m33.8     \u001b[39m | \u001b[39m8.024    \u001b[39m | \u001b[39m95.57    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 22, 'lambda_l1': 0.5291112242092539, 'lambda_l2': 1.5484673788684333, 'feature_fraction': 0.7073309699952618, 'bagging_fraction': 0.7609241608750359, 'min_child_samples': 26, 'min_child_weight': 28.853263494563777, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[2485]\tvalid_0's binary_logloss: 0.488801\nroc-auc : 0.7421354451712643\n\n| \u001b[35m5        \u001b[39m | \u001b[35m0.7421   \u001b[39m | \u001b[35m0.7609   \u001b[39m | \u001b[35m0.7073   \u001b[39m | \u001b[35m0.5291   \u001b[39m | \u001b[35m1.548    \u001b[39m | \u001b[35m25.53    \u001b[39m | \u001b[35m28.85    \u001b[39m | \u001b[35m21.5     \u001b[39m |\n하이퍼파라미터: {'num_leaves': 55, 'lambda_l1': 1.2338679937495138, 'lambda_l2': 1.8874961570292483, 'feature_fraction': 0.8060478613612108, 'bagging_fraction': 0.8088177485379385, 'min_child_samples': 36, 'min_child_weight': 18.615887128115514, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[1736]\tvalid_0's binary_logloss: 0.48886\nroc-auc : 0.7419427580805965\n\n| \u001b[39m6        \u001b[39m | \u001b[39m0.7419   \u001b[39m | \u001b[39m0.8088   \u001b[39m | \u001b[39m0.806    \u001b[39m | \u001b[39m1.234    \u001b[39m | \u001b[39m1.887    \u001b[39m | \u001b[39m35.68    \u001b[39m | \u001b[39m18.62    \u001b[39m | \u001b[39m54.96    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 45, 'lambda_l1': 1.3335334308913354, 'lambda_l2': 1.3412757392363188, 'feature_fraction': 0.5301127358146349, 'bagging_fraction': 0.8488155979636325, 'min_child_samples': 14, 'min_child_weight': 7.317388585087812, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[2168]\tvalid_0's binary_logloss: 0.488895\nroc-auc : 0.7418565089941668\n\n| \u001b[39m7        \u001b[39m | \u001b[39m0.7419   \u001b[39m | \u001b[39m0.8488   \u001b[39m | \u001b[39m0.5301   \u001b[39m | \u001b[39m1.334    \u001b[39m | \u001b[39m1.341    \u001b[39m | \u001b[39m14.47    \u001b[39m | \u001b[39m7.317    \u001b[39m | \u001b[39m45.23    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 33, 'lambda_l1': 0.8772030269246407, 'lambda_l2': 1.9767476761184524, 'feature_fraction': 0.7850983852089398, 'bagging_fraction': 0.6818553854713113, 'min_child_samples': 10, 'min_child_weight': 11.2349610486469, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[2500]\tvalid_0's binary_logloss: 0.488689\nroc-auc : 0.74223917249839\n\n| \u001b[35m8        \u001b[39m | \u001b[35m0.7422   \u001b[39m | \u001b[35m0.6819   \u001b[39m | \u001b[35m0.7851   \u001b[39m | \u001b[35m0.8772   \u001b[39m | \u001b[35m1.977    \u001b[39m | \u001b[35m9.592    \u001b[39m | \u001b[35m11.23    \u001b[39m | \u001b[35m32.9     \u001b[39m |\n하이퍼파라미터: {'num_leaves': 73, 'lambda_l1': 0.9326215457126126, 'lambda_l2': 0.4888511840032055, 'feature_fraction': 0.626645801269891, 'bagging_fraction': 0.8265541627326992, 'min_child_samples': 12, 'min_child_weight': 6.408381917050952, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[1884]\tvalid_0's binary_logloss: 0.488901\nroc-auc : 0.7418387188183043\n\n| \u001b[39m9        \u001b[39m | \u001b[39m0.7418   \u001b[39m | \u001b[39m0.8266   \u001b[39m | \u001b[39m0.6266   \u001b[39m | \u001b[39m0.9326   \u001b[39m | \u001b[39m0.4889   \u001b[39m | \u001b[39m12.15    \u001b[39m | \u001b[39m6.408    \u001b[39m | \u001b[39m72.51    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 28, 'lambda_l1': 0.7374503413219282, 'lambda_l2': 1.6419864596958702, 'feature_fraction': 0.5982911808400267, 'bagging_fraction': 0.5690914756743068, 'min_child_samples': 9, 'min_child_weight': 42.05930046744139, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[2499]\tvalid_0's binary_logloss: 0.488859\nroc-auc : 0.742058881869185\n\n| \u001b[39m10       \u001b[39m | \u001b[39m0.7421   \u001b[39m | \u001b[39m0.5691   \u001b[39m | \u001b[39m0.5983   \u001b[39m | \u001b[39m0.7375   \u001b[39m | \u001b[39m1.642    \u001b[39m | \u001b[39m9.37     \u001b[39m | \u001b[39m42.06    \u001b[39m | \u001b[39m27.69    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 21, 'lambda_l1': 0.34311954526680233, 'lambda_l2': 1.6593800527486526, 'feature_fraction': 0.7065446306993597, 'bagging_fraction': 0.7843401570869577, 'min_child_samples': 6, 'min_child_weight': 16.005328514836673, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[2485]\tvalid_0's binary_logloss: 0.488874\nroc-auc : 0.7420041560030413\n\n| \u001b[39m11       \u001b[39m | \u001b[39m0.742    \u001b[39m | \u001b[39m0.7843   \u001b[39m | \u001b[39m0.7065   \u001b[39m | \u001b[39m0.3431   \u001b[39m | \u001b[39m1.659    \u001b[39m | \u001b[39m5.873    \u001b[39m | \u001b[39m16.01    \u001b[39m | \u001b[39m21.05    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 34, 'lambda_l1': 0.9636123776351637, 'lambda_l2': 1.6798409033032238, 'feature_fraction': 0.933087393555541, 'bagging_fraction': 0.6836248173163968, 'min_child_samples': 13, 'min_child_weight': 25.870674715441677, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[2209]\tvalid_0's binary_logloss: 0.4887\nroc-auc : 0.7422491040797545\n\n| \u001b[35m12       \u001b[39m | \u001b[35m0.7422   \u001b[39m | \u001b[35m0.6836   \u001b[39m | \u001b[35m0.9331   \u001b[39m | \u001b[35m0.9636   \u001b[39m | \u001b[35m1.68     \u001b[39m | \u001b[35m12.92    \u001b[39m | \u001b[35m25.87    \u001b[39m | \u001b[35m33.69    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 34, 'lambda_l1': 1.390864534032433, 'lambda_l2': 0.5324235905455998, 'feature_fraction': 0.5846521991863145, 'bagging_fraction': 0.7075963468672685, 'min_child_samples': 12, 'min_child_weight': 25.436860616451565, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[2484]\tvalid_0's binary_logloss: 0.488771\nroc-auc : 0.7421484447731918\n\n| \u001b[39m13       \u001b[39m | \u001b[39m0.7421   \u001b[39m | \u001b[39m0.7076   \u001b[39m | \u001b[39m0.5847   \u001b[39m | \u001b[39m1.391    \u001b[39m | \u001b[39m0.5324   \u001b[39m | \u001b[39m12.49    \u001b[39m | \u001b[39m25.44    \u001b[39m | \u001b[39m33.79    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 33, 'lambda_l1': 0.0969869208333689, 'lambda_l2': 0.7447503577113781, 'feature_fraction': 0.8985554908743691, 'bagging_fraction': 0.7194153548980826, 'min_child_samples': 16, 'min_child_weight': 29.72215842857282, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[2340]\tvalid_0's binary_logloss: 0.488708\nroc-auc : 0.7422638479520598\n\n| \u001b[35m14       \u001b[39m | \u001b[35m0.7423   \u001b[39m | \u001b[35m0.7194   \u001b[39m | \u001b[35m0.8986   \u001b[39m | \u001b[35m0.09699  \u001b[39m | \u001b[35m0.7448   \u001b[39m | \u001b[35m15.69    \u001b[39m | \u001b[35m29.72    \u001b[39m | \u001b[35m33.14    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 32, 'lambda_l1': 0.3243178845030772, 'lambda_l2': 1.7106910204910113, 'feature_fraction': 0.7976996596756442, 'bagging_fraction': 0.753051616750049, 'min_child_samples': 21, 'min_child_weight': 20.82589140182686, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[2319]\tvalid_0's binary_logloss: 0.488772\nroc-auc : 0.7421468209334795\n\n| \u001b[39m15       \u001b[39m | \u001b[39m0.7421   \u001b[39m | \u001b[39m0.7531   \u001b[39m | \u001b[39m0.7977   \u001b[39m | \u001b[39m0.3243   \u001b[39m | \u001b[39m1.711    \u001b[39m | \u001b[39m21.49    \u001b[39m | \u001b[39m20.83    \u001b[39m | \u001b[39m31.8     \u001b[39m |\n하이퍼파라미터: {'num_leaves': 26, 'lambda_l1': 0.7672159028290175, 'lambda_l2': 1.953048418310788, 'feature_fraction': 0.8945782048804525, 'bagging_fraction': 0.6826912785013192, 'min_child_samples': 16, 'min_child_weight': 28.229317306772614, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[2500]\tvalid_0's binary_logloss: 0.488694\nroc-auc : 0.7422891087777351\n\n| \u001b[35m16       \u001b[39m | \u001b[35m0.7423   \u001b[39m | \u001b[35m0.6827   \u001b[39m | \u001b[35m0.8946   \u001b[39m | \u001b[35m0.7672   \u001b[39m | \u001b[35m1.953    \u001b[39m | \u001b[35m16.23    \u001b[39m | \u001b[35m28.23    \u001b[39m | \u001b[35m26.24    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 27, 'lambda_l1': 1.1586805088737042, 'lambda_l2': 1.4887751181225006, 'feature_fraction': 0.6668627958962754, 'bagging_fraction': 0.7878188290415087, 'min_child_samples': 12, 'min_child_weight': 1.0501884981861878, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[2499]\tvalid_0's binary_logloss: 0.48882\nroc-auc : 0.7420314977126761\n\n| \u001b[39m17       \u001b[39m | \u001b[39m0.742    \u001b[39m | \u001b[39m0.7878   \u001b[39m | \u001b[39m0.6669   \u001b[39m | \u001b[39m1.159    \u001b[39m | \u001b[39m1.489    \u001b[39m | \u001b[39m11.8     \u001b[39m | \u001b[39m1.05     \u001b[39m | \u001b[39m26.73    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 33, 'lambda_l1': 0.0008687019616937519, 'lambda_l2': 1.75423501083279, 'feature_fraction': 0.9567015503101549, 'bagging_fraction': 0.9046965838306968, 'min_child_samples': 27, 'min_child_weight': 41.51406223077819, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[2016]\tvalid_0's binary_logloss: 0.488817\nroc-auc : 0.7420966803168574\n\n| \u001b[39m18       \u001b[39m | \u001b[39m0.7421   \u001b[39m | \u001b[39m0.9047   \u001b[39m | \u001b[39m0.9567   \u001b[39m | \u001b[39m0.0008687\u001b[39m | \u001b[39m1.754    \u001b[39m | \u001b[39m26.63    \u001b[39m | \u001b[39m41.51    \u001b[39m | \u001b[39m33.12    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 21, 'lambda_l1': 0.4933403376732919, 'lambda_l2': 1.5414008009697195, 'feature_fraction': 0.8146977138897112, 'bagging_fraction': 0.5388154681451875, 'min_child_samples': 44, 'min_child_weight': 39.85385278742516, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[2498]\tvalid_0's binary_logloss: 0.488747\nroc-auc : 0.7422550762562039\n\n| \u001b[39m19       \u001b[39m | \u001b[39m0.7423   \u001b[39m | \u001b[39m0.5388   \u001b[39m | \u001b[39m0.8147   \u001b[39m | \u001b[39m0.4933   \u001b[39m | \u001b[39m1.541    \u001b[39m | \u001b[39m44.47    \u001b[39m | \u001b[39m39.85    \u001b[39m | \u001b[39m21.38    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 30, 'lambda_l1': 1.6358330952682574, 'lambda_l2': 1.6691775224422443, 'feature_fraction': 0.5127674018488373, 'bagging_fraction': 0.7419295291703376, 'min_child_samples': 49, 'min_child_weight': 47.16411700306736, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[2500]\tvalid_0's binary_logloss: 0.488933\nroc-auc : 0.7419523481126965\n\n| \u001b[39m20       \u001b[39m | \u001b[39m0.742    \u001b[39m | \u001b[39m0.7419   \u001b[39m | \u001b[39m0.5128   \u001b[39m | \u001b[39m1.636    \u001b[39m | \u001b[39m1.669    \u001b[39m | \u001b[39m49.17    \u001b[39m | \u001b[39m47.16    \u001b[39m | \u001b[39m29.99    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 21, 'lambda_l1': 1.6668876866811644, 'lambda_l2': 0.1018012465173419, 'feature_fraction': 0.7975049696761689, 'bagging_fraction': 0.6170872624291228, 'min_child_samples': 48, 'min_child_weight': 29.631168394002188, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[2499]\tvalid_0's binary_logloss: 0.488765\nroc-auc : 0.7421775544444829\n\n| \u001b[39m21       \u001b[39m | \u001b[39m0.7422   \u001b[39m | \u001b[39m0.6171   \u001b[39m | \u001b[39m0.7975   \u001b[39m | \u001b[39m1.667    \u001b[39m | \u001b[39m0.1018   \u001b[39m | \u001b[39m48.45    \u001b[39m | \u001b[39m29.63    \u001b[39m | \u001b[39m20.76    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 29, 'lambda_l1': 0.08868886563069367, 'lambda_l2': 1.4753108483231263, 'feature_fraction': 0.5050475049657487, 'bagging_fraction': 0.8555005938077673, 'min_child_samples': 40, 'min_child_weight': 32.35907541955912, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[2500]\tvalid_0's binary_logloss: 0.488899\nroc-auc : 0.7419061590038969\n\n| \u001b[39m22       \u001b[39m | \u001b[39m0.7419   \u001b[39m | \u001b[39m0.8555   \u001b[39m | \u001b[39m0.505    \u001b[39m | \u001b[39m0.08869  \u001b[39m | \u001b[39m1.475    \u001b[39m | \u001b[39m39.82    \u001b[39m | \u001b[39m32.36    \u001b[39m | \u001b[39m29.21    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 20, 'lambda_l1': 0.9349935084983558, 'lambda_l2': 1.1632931271091602, 'feature_fraction': 0.8647595504491836, 'bagging_fraction': 0.8430808076392882, 'min_child_samples': 33, 'min_child_weight': 46.995877875086535, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[2500]\tvalid_0's binary_logloss: 0.488765\nroc-auc : 0.7422406877530758\n\n| \u001b[39m23       \u001b[39m | \u001b[39m0.7422   \u001b[39m | \u001b[39m0.8431   \u001b[39m | \u001b[39m0.8648   \u001b[39m | \u001b[39m0.935    \u001b[39m | \u001b[39m1.163    \u001b[39m | \u001b[39m32.71    \u001b[39m | \u001b[39m47.0     \u001b[39m | \u001b[39m20.23    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 21, 'lambda_l1': 0.4437343485060181, 'lambda_l2': 0.8016192167108553, 'feature_fraction': 0.9147681021445362, 'bagging_fraction': 0.6546172224760095, 'min_child_samples': 22, 'min_child_weight': 43.104238347293155, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[2498]\tvalid_0's binary_logloss: 0.488709\nroc-auc : 0.7422779640054852\n\n| \u001b[39m24       \u001b[39m | \u001b[39m0.7423   \u001b[39m | \u001b[39m0.6546   \u001b[39m | \u001b[39m0.9148   \u001b[39m | \u001b[39m0.4437   \u001b[39m | \u001b[39m0.8016   \u001b[39m | \u001b[39m21.77    \u001b[39m | \u001b[39m43.1     \u001b[39m | \u001b[39m21.11    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 20, 'lambda_l1': 1.2258998713747045, 'lambda_l2': 0.4676147688384442, 'feature_fraction': 0.9114696177434294, 'bagging_fraction': 0.6638731581444175, 'min_child_samples': 49, 'min_child_weight': 4.732500011916842, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[2499]\tvalid_0's binary_logloss: 0.488733\nroc-auc : 0.7422126974946927\n\n| \u001b[39m25       \u001b[39m | \u001b[39m0.7422   \u001b[39m | \u001b[39m0.6639   \u001b[39m | \u001b[39m0.9115   \u001b[39m | \u001b[39m1.226    \u001b[39m | \u001b[39m0.4676   \u001b[39m | \u001b[39m49.09    \u001b[39m | \u001b[39m4.733    \u001b[39m | \u001b[39m20.36    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 34, 'lambda_l1': 0.3739107554968777, 'lambda_l2': 0.5113807146138529, 'feature_fraction': 0.8448967633928377, 'bagging_fraction': 0.921155062508757, 'min_child_samples': 50, 'min_child_weight': 6.0388810073731785, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[2499]\tvalid_0's binary_logloss: 0.48888\nroc-auc : 0.7419315827071119\n\n| \u001b[39m26       \u001b[39m | \u001b[39m0.7419   \u001b[39m | \u001b[39m0.9212   \u001b[39m | \u001b[39m0.8449   \u001b[39m | \u001b[39m0.3739   \u001b[39m | \u001b[39m0.5114   \u001b[39m | \u001b[39m49.67    \u001b[39m | \u001b[39m6.039    \u001b[39m | \u001b[39m33.92    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 22, 'lambda_l1': 1.1843086191646766, 'lambda_l2': 0.6087445651566281, 'feature_fraction': 0.6070126507168884, 'bagging_fraction': 0.8225216453464346, 'min_child_samples': 35, 'min_child_weight': 5.409426899322119, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[2500]\tvalid_0's binary_logloss: 0.488918\nroc-auc : 0.7419302086129603\n\n| \u001b[39m27       \u001b[39m | \u001b[39m0.7419   \u001b[39m | \u001b[39m0.8225   \u001b[39m | \u001b[39m0.607    \u001b[39m | \u001b[39m1.184    \u001b[39m | \u001b[39m0.6087   \u001b[39m | \u001b[39m34.77    \u001b[39m | \u001b[39m5.409    \u001b[39m | \u001b[39m21.83    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 23, 'lambda_l1': 1.459446880850262, 'lambda_l2': 1.3147120025265755, 'feature_fraction': 0.8597148882863523, 'bagging_fraction': 0.8216060491303654, 'min_child_samples': 24, 'min_child_weight': 49.95974410321938, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[2499]\tvalid_0's binary_logloss: 0.488734\nroc-auc : 0.7422804436926325\n\n| \u001b[39m28       \u001b[39m | \u001b[39m0.7423   \u001b[39m | \u001b[39m0.8216   \u001b[39m | \u001b[39m0.8597   \u001b[39m | \u001b[39m1.459    \u001b[39m | \u001b[39m1.315    \u001b[39m | \u001b[39m24.01    \u001b[39m | \u001b[39m49.96    \u001b[39m | \u001b[39m23.39    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 53, 'lambda_l1': 1.1360337820039672, 'lambda_l2': 1.0925344500174339, 'feature_fraction': 0.9071028012768081, 'bagging_fraction': 0.7057801127555118, 'min_child_samples': 5, 'min_child_weight': 49.935450602667885, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[1654]\tvalid_0's binary_logloss: 0.488806\nroc-auc : 0.7421697817308696\n\n| \u001b[39m29       \u001b[39m | \u001b[39m0.7422   \u001b[39m | \u001b[39m0.7058   \u001b[39m | \u001b[39m0.9071   \u001b[39m | \u001b[39m1.136    \u001b[39m | \u001b[39m1.093    \u001b[39m | \u001b[39m5.392    \u001b[39m | \u001b[39m49.94    \u001b[39m | \u001b[39m52.72    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 71, 'lambda_l1': 0.23187348329109247, 'lambda_l2': 0.987260490067468, 'feature_fraction': 0.9870810273206517, 'bagging_fraction': 0.8958281257308982, 'min_child_samples': 6, 'min_child_weight': 49.679926086238524, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[1517]\tvalid_0's binary_logloss: 0.488957\nroc-auc : 0.7418205525433961\n\n| \u001b[39m30       \u001b[39m | \u001b[39m0.7418   \u001b[39m | \u001b[39m0.8958   \u001b[39m | \u001b[39m0.9871   \u001b[39m | \u001b[39m0.2319   \u001b[39m | \u001b[39m0.9873   \u001b[39m | \u001b[39m5.796    \u001b[39m | \u001b[39m49.68    \u001b[39m | \u001b[39m71.0     \u001b[39m |\n하이퍼파라미터: {'num_leaves': 47, 'lambda_l1': 0.180814661728399, 'lambda_l2': 1.4405845479054684, 'feature_fraction': 0.7048048206953028, 'bagging_fraction': 0.790543679794255, 'min_child_samples': 6, 'min_child_weight': 36.52933798733949, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[1980]\tvalid_0's binary_logloss: 0.488783\nroc-auc : 0.7421831623675255\n\n| \u001b[39m31       \u001b[39m | \u001b[39m0.7422   \u001b[39m | \u001b[39m0.7905   \u001b[39m | \u001b[39m0.7048   \u001b[39m | \u001b[39m0.1808   \u001b[39m | \u001b[39m1.441    \u001b[39m | \u001b[39m5.795    \u001b[39m | \u001b[39m36.53    \u001b[39m | \u001b[39m47.11    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 42, 'lambda_l1': 0.12482737639377595, 'lambda_l2': 1.1337939264970134, 'feature_fraction': 0.650272060476524, 'bagging_fraction': 0.7428236389020488, 'min_child_samples': 13, 'min_child_weight': 49.67577440085631, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[2346]\tvalid_0's binary_logloss: 0.488844\nroc-auc : 0.7421007305383402\n\n| \u001b[39m32       \u001b[39m | \u001b[39m0.7421   \u001b[39m | \u001b[39m0.7428   \u001b[39m | \u001b[39m0.6503   \u001b[39m | \u001b[39m0.1248   \u001b[39m | \u001b[39m1.134    \u001b[39m | \u001b[39m13.49    \u001b[39m | \u001b[39m49.68    \u001b[39m | \u001b[39m41.85    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 68, 'lambda_l1': 0.3684391653738781, 'lambda_l2': 1.703093939989758, 'feature_fraction': 0.9836090600714171, 'bagging_fraction': 0.6794128475315616, 'min_child_samples': 50, 'min_child_weight': 49.83431308157731, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[1419]\tvalid_0's binary_logloss: 0.488845\nroc-auc : 0.7421031737014331\n\n| \u001b[39m33       \u001b[39m | \u001b[39m0.7421   \u001b[39m | \u001b[39m0.6794   \u001b[39m | \u001b[39m0.9836   \u001b[39m | \u001b[39m0.3684   \u001b[39m | \u001b[39m1.703    \u001b[39m | \u001b[39m49.68    \u001b[39m | \u001b[39m49.83    \u001b[39m | \u001b[39m67.63    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 21, 'lambda_l1': 1.5800482731287964, 'lambda_l2': 0.8911927594955096, 'feature_fraction': 0.7521066654049173, 'bagging_fraction': 0.9845366360379537, 'min_child_samples': 50, 'min_child_weight': 15.682863307959936, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[2495]\tvalid_0's binary_logloss: 0.488871\nroc-auc : 0.7420843865176014\n\n| \u001b[39m34       \u001b[39m | \u001b[39m0.7421   \u001b[39m | \u001b[39m0.9845   \u001b[39m | \u001b[39m0.7521   \u001b[39m | \u001b[39m1.58     \u001b[39m | \u001b[39m0.8912   \u001b[39m | \u001b[39m49.73    \u001b[39m | \u001b[39m15.68    \u001b[39m | \u001b[39m21.04    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 68, 'lambda_l1': 1.1143060856665206, 'lambda_l2': 0.857811692291313, 'feature_fraction': 0.6602419537602094, 'bagging_fraction': 0.6012784375323318, 'min_child_samples': 50, 'min_child_weight': 3.998006247588643, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[1823]\tvalid_0's binary_logloss: 0.488816\nroc-auc : 0.7419427116851762\n\n| \u001b[39m35       \u001b[39m | \u001b[39m0.7419   \u001b[39m | \u001b[39m0.6013   \u001b[39m | \u001b[39m0.6602   \u001b[39m | \u001b[39m1.114    \u001b[39m | \u001b[39m0.8578   \u001b[39m | \u001b[39m49.65    \u001b[39m | \u001b[39m3.998    \u001b[39m | \u001b[39m67.69    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 57, 'lambda_l1': 0.8495946254970914, 'lambda_l2': 0.26846727223067646, 'feature_fraction': 0.5777163494370949, 'bagging_fraction': 0.6329715116196348, 'min_child_samples': 36, 'min_child_weight': 49.17164898928825, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[1950]\tvalid_0's binary_logloss: 0.488942\nroc-auc : 0.7419432269704831\n\n| \u001b[39m36       \u001b[39m | \u001b[39m0.7419   \u001b[39m | \u001b[39m0.633    \u001b[39m | \u001b[39m0.5777   \u001b[39m | \u001b[39m0.8496   \u001b[39m | \u001b[39m0.2685   \u001b[39m | \u001b[39m35.53    \u001b[39m | \u001b[39m49.17    \u001b[39m | \u001b[39m57.04    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 98, 'lambda_l1': 0.5287324776176641, 'lambda_l2': 0.8436999918444215, 'feature_fraction': 0.8721737480885186, 'bagging_fraction': 0.6695662815385393, 'min_child_samples': 6, 'min_child_weight': 49.10568882417518, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[1316]\tvalid_0's binary_logloss: 0.489018\nroc-auc : 0.7417994791511913\n\n| \u001b[39m37       \u001b[39m | \u001b[39m0.7418   \u001b[39m | \u001b[39m0.6696   \u001b[39m | \u001b[39m0.8722   \u001b[39m | \u001b[39m0.5287   \u001b[39m | \u001b[39m0.8437   \u001b[39m | \u001b[39m5.532    \u001b[39m | \u001b[39m49.11    \u001b[39m | \u001b[39m97.85    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 65, 'lambda_l1': 0.5273678115639335, 'lambda_l2': 1.6598330097106393, 'feature_fraction': 0.8456139131768421, 'bagging_fraction': 0.5743675452498136, 'min_child_samples': 49, 'min_child_weight': 32.35364149773911, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[1648]\tvalid_0's binary_logloss: 0.488841\nroc-auc : 0.7420500420609035\n\n| \u001b[39m38       \u001b[39m | \u001b[39m0.7421   \u001b[39m | \u001b[39m0.5744   \u001b[39m | \u001b[39m0.8456   \u001b[39m | \u001b[39m0.5274   \u001b[39m | \u001b[39m1.66     \u001b[39m | \u001b[39m49.34    \u001b[39m | \u001b[39m32.35    \u001b[39m | \u001b[39m65.27    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 20, 'lambda_l1': 0.8798827764767814, 'lambda_l2': 1.6301770765273662, 'feature_fraction': 0.9867755856864446, 'bagging_fraction': 0.6576966562162581, 'min_child_samples': 13, 'min_child_weight': 34.01638896512603, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[2499]\tvalid_0's binary_logloss: 0.488677\nroc-auc : 0.7423408761954793\n\n| \u001b[35m39       \u001b[39m | \u001b[35m0.7423   \u001b[39m | \u001b[35m0.6577   \u001b[39m | \u001b[35m0.9868   \u001b[39m | \u001b[35m0.8799   \u001b[39m | \u001b[35m1.63     \u001b[39m | \u001b[35m12.59    \u001b[39m | \u001b[35m34.02    \u001b[39m | \u001b[35m20.05    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 23, 'lambda_l1': 0.02116565542597626, 'lambda_l2': 1.2926856890532765, 'feature_fraction': 0.9604253821935917, 'bagging_fraction': 0.5881487972773611, 'min_child_samples': 5, 'min_child_weight': 30.387003103108373, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[2472]\tvalid_0's binary_logloss: 0.488696\nroc-auc : 0.7422663039479285\n\n| \u001b[39m40       \u001b[39m | \u001b[39m0.7423   \u001b[39m | \u001b[39m0.5881   \u001b[39m | \u001b[39m0.9604   \u001b[39m | \u001b[39m0.02117  \u001b[39m | \u001b[39m1.293    \u001b[39m | \u001b[39m5.083    \u001b[39m | \u001b[39m30.39    \u001b[39m | \u001b[39m22.95    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 95, 'lambda_l1': 0.25894885405879786, 'lambda_l2': 1.8902069081676243, 'feature_fraction': 0.699841941413195, 'bagging_fraction': 0.9852466154230448, 'min_child_samples': 5, 'min_child_weight': 2.3820124188240084, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[1654]\tvalid_0's binary_logloss: 0.489097\nroc-auc : 0.7414745394469757\n\n| \u001b[39m41       \u001b[39m | \u001b[39m0.7415   \u001b[39m | \u001b[39m0.9852   \u001b[39m | \u001b[39m0.6998   \u001b[39m | \u001b[39m0.2589   \u001b[39m | \u001b[39m1.89     \u001b[39m | \u001b[39m5.039    \u001b[39m | \u001b[39m2.382    \u001b[39m | \u001b[39m95.29    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 57, 'lambda_l1': 1.0996904180377252, 'lambda_l2': 1.8043730383236665, 'feature_fraction': 0.9184858515058929, 'bagging_fraction': 0.8290050137645432, 'min_child_samples': 5, 'min_child_weight': 28.76824308251777, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[1569]\tvalid_0's binary_logloss: 0.488869\nroc-auc : 0.7419863411487636\n\n| \u001b[39m42       \u001b[39m | \u001b[39m0.742    \u001b[39m | \u001b[39m0.829    \u001b[39m | \u001b[39m0.9185   \u001b[39m | \u001b[39m1.1      \u001b[39m | \u001b[39m1.804    \u001b[39m | \u001b[39m5.156    \u001b[39m | \u001b[39m28.77    \u001b[39m | \u001b[39m57.23    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 20, 'lambda_l1': 0.8284748854040411, 'lambda_l2': 1.273152717146933, 'feature_fraction': 0.6248217964061988, 'bagging_fraction': 0.8237779564482356, 'min_child_samples': 46, 'min_child_weight': 46.872510737991156, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[2499]\tvalid_0's binary_logloss: 0.488897\nroc-auc : 0.7420068232461431\n\n| \u001b[39m43       \u001b[39m | \u001b[39m0.742    \u001b[39m | \u001b[39m0.8238   \u001b[39m | \u001b[39m0.6248   \u001b[39m | \u001b[39m0.8285   \u001b[39m | \u001b[39m1.273    \u001b[39m | \u001b[39m46.46    \u001b[39m | \u001b[39m46.87    \u001b[39m | \u001b[39m20.04    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 20, 'lambda_l1': 1.0178226824971661, 'lambda_l2': 0.14136872598493855, 'feature_fraction': 0.7717991243157059, 'bagging_fraction': 0.82325029426907, 'min_child_samples': 14, 'min_child_weight': 49.26336993517773, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[2499]\tvalid_0's binary_logloss: 0.488804\nroc-auc : 0.7421878295493853\n\n| \u001b[39m44       \u001b[39m | \u001b[39m0.7422   \u001b[39m | \u001b[39m0.8233   \u001b[39m | \u001b[39m0.7718   \u001b[39m | \u001b[39m1.018    \u001b[39m | \u001b[39m0.1414   \u001b[39m | \u001b[39m13.61    \u001b[39m | \u001b[39m49.26    \u001b[39m | \u001b[39m20.2     \u001b[39m |\n하이퍼파라미터: {'num_leaves': 42, 'lambda_l1': 0.22738330145610552, 'lambda_l2': 1.7821642914953126, 'feature_fraction': 0.7596173245020675, 'bagging_fraction': 0.964352327252304, 'min_child_samples': 17, 'min_child_weight': 29.525138422474935, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[2268]\tvalid_0's binary_logloss: 0.48889\nroc-auc : 0.741972270503622\n\n| \u001b[39m45       \u001b[39m | \u001b[39m0.742    \u001b[39m | \u001b[39m0.9644   \u001b[39m | \u001b[39m0.7596   \u001b[39m | \u001b[39m0.2274   \u001b[39m | \u001b[39m1.782    \u001b[39m | \u001b[39m16.79    \u001b[39m | \u001b[39m29.53    \u001b[39m | \u001b[39m41.89    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 21, 'lambda_l1': 1.2251341644461038, 'lambda_l2': 0.019336798496412122, 'feature_fraction': 0.7037681948797749, 'bagging_fraction': 0.8896338971454856, 'min_child_samples': 36, 'min_child_weight': 36.83352240297044, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[2499]\tvalid_0's binary_logloss: 0.488864\nroc-auc : 0.7420578355043856\n\n| \u001b[39m46       \u001b[39m | \u001b[39m0.7421   \u001b[39m | \u001b[39m0.8896   \u001b[39m | \u001b[39m0.7038   \u001b[39m | \u001b[39m1.225    \u001b[39m | \u001b[39m0.01934  \u001b[39m | \u001b[39m36.05    \u001b[39m | \u001b[39m36.83    \u001b[39m | \u001b[39m20.52    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 32, 'lambda_l1': 0.12365931116742002, 'lambda_l2': 1.8847998821750824, 'feature_fraction': 0.8828297423537881, 'bagging_fraction': 0.5024417571224058, 'min_child_samples': 6, 'min_child_weight': 33.74412877738659, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[2331]\tvalid_0's binary_logloss: 0.488673\nroc-auc : 0.742353919231417\n\n| \u001b[35m47       \u001b[39m | \u001b[35m0.7424   \u001b[39m | \u001b[35m0.5024   \u001b[39m | \u001b[35m0.8828   \u001b[39m | \u001b[35m0.1237   \u001b[39m | \u001b[35m1.885    \u001b[39m | \u001b[35m6.346    \u001b[39m | \u001b[35m33.74    \u001b[39m | \u001b[35m32.23    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 29, 'lambda_l1': 0.8707943604612536, 'lambda_l2': 1.7420158745183005, 'feature_fraction': 0.9598803616657643, 'bagging_fraction': 0.8479243912038525, 'min_child_samples': 10, 'min_child_weight': 30.90927580041496, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[2500]\tvalid_0's binary_logloss: 0.488761\nroc-auc : 0.7421799531864285\n\n| \u001b[39m48       \u001b[39m | \u001b[39m0.7422   \u001b[39m | \u001b[39m0.8479   \u001b[39m | \u001b[39m0.9599   \u001b[39m | \u001b[39m0.8708   \u001b[39m | \u001b[39m1.742    \u001b[39m | \u001b[39m9.835    \u001b[39m | \u001b[39m30.91    \u001b[39m | \u001b[39m28.72    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 39, 'lambda_l1': 0.4546611921058159, 'lambda_l2': 1.9580410052669384, 'feature_fraction': 0.7050658333712755, 'bagging_fraction': 0.5954853362287003, 'min_child_samples': 5, 'min_child_weight': 35.58350807154318, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[2170]\tvalid_0's binary_logloss: 0.48878\nroc-auc : 0.742148582972316\n\n| \u001b[39m49       \u001b[39m | \u001b[39m0.7421   \u001b[39m | \u001b[39m0.5955   \u001b[39m | \u001b[39m0.7051   \u001b[39m | \u001b[39m0.4547   \u001b[39m | \u001b[39m1.958    \u001b[39m | \u001b[39m5.206    \u001b[39m | \u001b[39m35.58    \u001b[39m | \u001b[39m38.86    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 23, 'lambda_l1': 0.27484819728426135, 'lambda_l2': 0.16928146489417828, 'feature_fraction': 0.523672016523448, 'bagging_fraction': 0.5649334728023769, 'min_child_samples': 18, 'min_child_weight': 34.51617573657375, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[2492]\tvalid_0's binary_logloss: 0.488931\nroc-auc : 0.7419367849170106\n\n| \u001b[39m50       \u001b[39m | \u001b[39m0.7419   \u001b[39m | \u001b[39m0.5649   \u001b[39m | \u001b[39m0.5237   \u001b[39m | \u001b[39m0.2748   \u001b[39m | \u001b[39m0.1693   \u001b[39m | \u001b[39m18.48    \u001b[39m | \u001b[39m34.52    \u001b[39m | \u001b[39m22.74    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 21, 'lambda_l1': 0.19782005238456257, 'lambda_l2': 1.0889929113345669, 'feature_fraction': 0.8956196052578439, 'bagging_fraction': 0.6293335808859557, 'min_child_samples': 12, 'min_child_weight': 27.31220911865022, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[2500]\tvalid_0's binary_logloss: 0.488734\nroc-auc : 0.742250040872391\n\n| \u001b[39m51       \u001b[39m | \u001b[39m0.7423   \u001b[39m | \u001b[39m0.6293   \u001b[39m | \u001b[39m0.8956   \u001b[39m | \u001b[39m0.1978   \u001b[39m | \u001b[39m1.089    \u001b[39m | \u001b[39m11.88    \u001b[39m | \u001b[39m27.31    \u001b[39m | \u001b[39m21.36    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 20, 'lambda_l1': 1.1960951405528994, 'lambda_l2': 1.9415460794547161, 'feature_fraction': 0.7683726783246443, 'bagging_fraction': 0.530512583658731, 'min_child_samples': 7, 'min_child_weight': 35.95823871115363, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[2498]\tvalid_0's binary_logloss: 0.488772\nroc-auc : 0.7421862866548746\n\n| \u001b[39m52       \u001b[39m | \u001b[39m0.7422   \u001b[39m | \u001b[39m0.5305   \u001b[39m | \u001b[39m0.7684   \u001b[39m | \u001b[39m1.196    \u001b[39m | \u001b[39m1.942    \u001b[39m | \u001b[39m7.277    \u001b[39m | \u001b[39m35.96    \u001b[39m | \u001b[39m20.01    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 23, 'lambda_l1': 0.6931482747688176, 'lambda_l2': 0.9002973175786424, 'feature_fraction': 0.7799861979690691, 'bagging_fraction': 0.6021363476769818, 'min_child_samples': 27, 'min_child_weight': 45.41135492741065, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[2500]\tvalid_0's binary_logloss: 0.48876\nroc-auc : 0.7422558679397596\n\n| \u001b[39m53       \u001b[39m | \u001b[39m0.7423   \u001b[39m | \u001b[39m0.6021   \u001b[39m | \u001b[39m0.78     \u001b[39m | \u001b[39m0.6931   \u001b[39m | \u001b[39m0.9003   \u001b[39m | \u001b[39m27.21    \u001b[39m | \u001b[39m45.41    \u001b[39m | \u001b[39m23.38    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 25, 'lambda_l1': 0.7275800018435874, 'lambda_l2': 1.4918751432912647, 'feature_fraction': 0.5612299718276645, 'bagging_fraction': 0.689662582750072, 'min_child_samples': 18, 'min_child_weight': 21.845419310381388, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[2500]\tvalid_0's binary_logloss: 0.488837\nroc-auc : 0.7420385567265255\n\n| \u001b[39m54       \u001b[39m | \u001b[39m0.742    \u001b[39m | \u001b[39m0.6897   \u001b[39m | \u001b[39m0.5612   \u001b[39m | \u001b[39m0.7276   \u001b[39m | \u001b[39m1.492    \u001b[39m | \u001b[39m18.34    \u001b[39m | \u001b[39m21.85    \u001b[39m | \u001b[39m24.52    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 28, 'lambda_l1': 0.058512580660607716, 'lambda_l2': 0.3531342795964272, 'feature_fraction': 0.6168017706112464, 'bagging_fraction': 0.7467635078348613, 'min_child_samples': 18, 'min_child_weight': 49.059518021067994, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[2499]\tvalid_0's binary_logloss: 0.488818\nroc-auc : 0.742171113378147\n\n| \u001b[39m55       \u001b[39m | \u001b[39m0.7422   \u001b[39m | \u001b[39m0.7468   \u001b[39m | \u001b[39m0.6168   \u001b[39m | \u001b[39m0.05851  \u001b[39m | \u001b[39m0.3531   \u001b[39m | \u001b[39m18.42    \u001b[39m | \u001b[39m49.06    \u001b[39m | \u001b[39m28.06    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 27, 'lambda_l1': 1.1389050272339163, 'lambda_l2': 0.05533291839242094, 'feature_fraction': 0.9004342274139244, 'bagging_fraction': 0.6122241223307764, 'min_child_samples': 31, 'min_child_weight': 49.99417080217577, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[2489]\tvalid_0's binary_logloss: 0.488717\nroc-auc : 0.7423112245861927\n\n| \u001b[39m56       \u001b[39m | \u001b[39m0.7423   \u001b[39m | \u001b[39m0.6122   \u001b[39m | \u001b[39m0.9004   \u001b[39m | \u001b[39m1.139    \u001b[39m | \u001b[39m0.05533  \u001b[39m | \u001b[39m31.33    \u001b[39m | \u001b[39m49.99    \u001b[39m | \u001b[39m26.74    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 34, 'lambda_l1': 1.4002506884046717, 'lambda_l2': 0.9882787125170824, 'feature_fraction': 0.6009713217609405, 'bagging_fraction': 0.7403328685658616, 'min_child_samples': 32, 'min_child_weight': 49.959905742690296, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[2499]\tvalid_0's binary_logloss: 0.488823\nroc-auc : 0.7421189471572154\n\n| \u001b[39m57       \u001b[39m | \u001b[39m0.7421   \u001b[39m | \u001b[39m0.7403   \u001b[39m | \u001b[39m0.601    \u001b[39m | \u001b[39m1.4      \u001b[39m | \u001b[39m0.9883   \u001b[39m | \u001b[39m31.5     \u001b[39m | \u001b[39m49.96    \u001b[39m | \u001b[39m34.31    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 35, 'lambda_l1': 1.4920707635569916, 'lambda_l2': 1.7994343797880221, 'feature_fraction': 0.7866613834778131, 'bagging_fraction': 0.7888735875433872, 'min_child_samples': 5, 'min_child_weight': 30.217625398786097, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[2313]\tvalid_0's binary_logloss: 0.488772\nroc-auc : 0.7421361628195748\n\n| \u001b[39m58       \u001b[39m | \u001b[39m0.7421   \u001b[39m | \u001b[39m0.7889   \u001b[39m | \u001b[39m0.7867   \u001b[39m | \u001b[39m1.492    \u001b[39m | \u001b[39m1.799    \u001b[39m | \u001b[39m5.408    \u001b[39m | \u001b[39m30.22    \u001b[39m | \u001b[39m35.07    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 26, 'lambda_l1': 0.255276557768255, 'lambda_l2': 1.2398197940721112, 'feature_fraction': 0.5264796469918629, 'bagging_fraction': 0.9407420224848662, 'min_child_samples': 37, 'min_child_weight': 46.8856026767501, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[2499]\tvalid_0's binary_logloss: 0.488951\nroc-auc : 0.7419391471349022\n\n| \u001b[39m59       \u001b[39m | \u001b[39m0.7419   \u001b[39m | \u001b[39m0.9407   \u001b[39m | \u001b[39m0.5265   \u001b[39m | \u001b[39m0.2553   \u001b[39m | \u001b[39m1.24     \u001b[39m | \u001b[39m36.64    \u001b[39m | \u001b[39m46.89    \u001b[39m | \u001b[39m26.11    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 29, 'lambda_l1': 1.8117784505201937, 'lambda_l2': 0.0698990913838049, 'feature_fraction': 0.8747106750697671, 'bagging_fraction': 0.8425226660786711, 'min_child_samples': 26, 'min_child_weight': 49.877598666769565, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 951\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 221\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[2499]\tvalid_0's binary_logloss: 0.488731\nroc-auc : 0.7422423086313783\n\n| \u001b[39m60       \u001b[39m | \u001b[39m0.7422   \u001b[39m | \u001b[39m0.8425   \u001b[39m | \u001b[39m0.8747   \u001b[39m | \u001b[39m1.812    \u001b[39m | \u001b[39m0.0699   \u001b[39m | \u001b[39m26.12    \u001b[39m | \u001b[39m49.88    \u001b[39m | \u001b[39m28.87    \u001b[39m |\n=============================================================================================================\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# 평가함수 점수가 최대일 때 하이퍼파라미터\nmax_params = optimizer.max['params']\nmax_params","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T12:29:43.897298Z","iopub.execute_input":"2025-02-11T12:29:43.897574Z","iopub.status.idle":"2025-02-11T12:29:43.903728Z","shell.execute_reply.started":"2025-02-11T12:29:43.897550Z","shell.execute_reply":"2025-02-11T12:29:43.903051Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"{'bagging_fraction': 0.5024417571224058,\n 'feature_fraction': 0.8828297423537881,\n 'lambda_l1': 0.12365931116742002,\n 'lambda_l2': 1.8847998821750824,\n 'min_child_samples': 6.346129551978789,\n 'min_child_weight': 33.74412877738659,\n 'num_leaves': 32.22730568024136}"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"# 정수형 하이퍼파라미터 변환\nmax_params['num_leaves'] = int(round(max_params['num_leaves']))\nmax_params['min_child_samples'] = int(round(max_params['min_child_samples']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T12:29:43.904580Z","iopub.execute_input":"2025-02-11T12:29:43.904890Z","iopub.status.idle":"2025-02-11T12:29:43.916873Z","shell.execute_reply.started":"2025-02-11T12:29:43.904860Z","shell.execute_reply":"2025-02-11T12:29:43.916163Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"# 값이 고정된 하이퍼파라미터 추가\nmax_params.update(fixed_params)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T12:29:43.917734Z","iopub.execute_input":"2025-02-11T12:29:43.918014Z","iopub.status.idle":"2025-02-11T12:29:43.933909Z","shell.execute_reply.started":"2025-02-11T12:29:43.917986Z","shell.execute_reply":"2025-02-11T12:29:43.933120Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"max_params","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T12:29:43.934775Z","iopub.execute_input":"2025-02-11T12:29:43.935018Z","iopub.status.idle":"2025-02-11T12:29:43.949441Z","shell.execute_reply.started":"2025-02-11T12:29:43.934988Z","shell.execute_reply":"2025-02-11T12:29:43.948620Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"{'bagging_fraction': 0.5024417571224058,\n 'feature_fraction': 0.8828297423537881,\n 'lambda_l1': 0.12365931116742002,\n 'lambda_l2': 1.8847998821750824,\n 'min_child_samples': 6,\n 'min_child_weight': 33.74412877738659,\n 'num_leaves': 32,\n 'objective': 'binary',\n 'learning_rate': 0.005,\n 'bagging_freq': 1,\n 'force_row_wise': True,\n 'random_state': 1991}"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"def lgb_roc_auc(y_pred, dataset):\n    y_true = dataset.get_label()\n    return \"roc_auc\", roc_auc_score(y_true, y_pred), True  # (지표 이름, 값, 높은 값이 더 좋은지 여부)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T12:37:12.256337Z","iopub.execute_input":"2025-02-11T12:37:12.256656Z","iopub.status.idle":"2025-02-11T12:37:12.260611Z","shell.execute_reply.started":"2025-02-11T12:37:12.256632Z","shell.execute_reply":"2025-02-11T12:37:12.259694Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\n\n# 층화 K 폴드 교차 검증기 생성\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1991)\n\n# OOF 방식으로 훈련된 모델로 검증 데이터 타깃값을 예측한 확률을 담을 1차원 배열\noof_val_preds = np.zeros(X.shape[0]) \n# OOF 방식으로 훈련된 모델로 테스트 데이터 타깃값을 예측한 확률을 담을 1차원 배열\noof_test_preds = np.zeros(X_test.shape[0]) \n\n# OOF 방식으로 모델 훈련, 검증, 예측\nfor idx, (train_idx, valid_idx) in enumerate(folds.split(X, y)):\n    # 각 폴드를 구분하는 문구 출력\n    print('#'*40, f'폴드 {idx+1} / 폴드 {folds.n_splits}', '#'*40)\n    \n    # 훈련용 데이터, 검증용 데이터 설정\n    X_train, y_train = X[train_idx], y[train_idx] # 훈련용 데이터\n    X_valid, y_valid = X[valid_idx], y[valid_idx] # 검증용 데이터\n\n    # LightGBM 전용 데이터셋 생성\n    dtrain = lgb.Dataset(X_train, y_train) # LightGBM 전용 훈련 데이터셋\n    dvalid = lgb.Dataset(X_valid, y_valid) # LightGBM 전용 검증 데이터셋\n                          \n    # LightGBM 모델 훈련\n    lgb_model = lgb.train(params=max_params,    # 최적 하이퍼파라미터\n                          train_set=dtrain,     # 훈련 데이터셋\n                          num_boost_round=2500, # 부스팅 반복 횟수\n                          valid_sets=dvalid,    # 성능 평가용 검증 데이터셋\n                          feval = lgb_roc_auc,\n                          callbacks=[early_stopping(stopping_rounds=100)])\n    \n    # 테스트 데이터를 활용해 OOF 예측\n    oof_test_preds += lgb_model.predict(X_test)/folds.n_splits\n    # 모델 성능 평가를 위한 검증 데이터 타깃값 예측 \n    oof_val_preds[valid_idx] += lgb_model.predict(X_valid)\n    \n    # 검증 데이터 예측 확률에 대한 ROC-AUC\n    roc_auc = roc_auc_score(y_valid, oof_val_preds[valid_idx])\n    print(f'폴드 {idx+1} roc-auc : {roc_auc}\\n')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T12:37:30.831641Z","iopub.execute_input":"2025-02-11T12:37:30.831926Z","iopub.status.idle":"2025-02-11T12:44:18.538153Z","shell.execute_reply.started":"2025-02-11T12:37:30.831906Z","shell.execute_reply":"2025-02-11T12:44:18.537351Z"}},"outputs":[{"name":"stdout","text":"######################################## 폴드 1 / 폴드 5 ########################################\n[LightGBM] [Info] Number of positive: 52982, number of negative: 152098\n[LightGBM] [Info] Total Bins 904\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 198\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n[LightGBM] [Info] Start training from score -1.054573\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[1889]\tvalid_0's binary_logloss: 0.487434\tvalid_0's roc_auc: 0.740021\n폴드 1 roc-auc : 0.7400205130587598\n\n######################################## 폴드 2 / 폴드 5 ########################################\n[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n[LightGBM] [Info] Total Bins 913\n[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 200\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n[LightGBM] [Info] Start training from score -1.054554\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[2016]\tvalid_0's binary_logloss: 0.489382\tvalid_0's roc_auc: 0.736455\n폴드 2 roc-auc : 0.736454921150452\n\n######################################## 폴드 3 / 폴드 5 ########################################\n[LightGBM] [Info] Number of positive: 52983, number of negative: 152098\n[LightGBM] [Info] Total Bins 916\n[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 200\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054554\n[LightGBM] [Info] Start training from score -1.054554\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[1870]\tvalid_0's binary_logloss: 0.488112\tvalid_0's roc_auc: 0.739147\n폴드 3 roc-auc : 0.7391472519246716\n\n######################################## 폴드 4 / 폴드 5 ########################################\n[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n[LightGBM] [Info] Total Bins 915\n[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 201\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n[LightGBM] [Info] Start training from score -1.054579\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[1419]\tvalid_0's binary_logloss: 0.487361\tvalid_0's roc_auc: 0.742153\n폴드 4 roc-auc : 0.7421527743517855\n\n######################################## 폴드 5 / 폴드 5 ########################################\n[LightGBM] [Info] Number of positive: 52982, number of negative: 152099\n[LightGBM] [Info] Total Bins 910\n[LightGBM] [Info] Number of data points in the train set: 205081, number of used features: 200\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258347 -> initscore=-1.054579\n[LightGBM] [Info] Start training from score -1.054579\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[1712]\tvalid_0's binary_logloss: 0.48636\tvalid_0's roc_auc: 0.743485\n폴드 5 roc-auc : 0.7434847068782325\n\n","output_type":"stream"}],"execution_count":42},{"cell_type":"markdown","source":"## 5. 최종 결과","metadata":{}},{"cell_type":"code","source":"print('OOF 검증 데이터 roc-auc :', roc_auc_score(y, oof_val_preds))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T12:44:52.109194Z","iopub.execute_input":"2025-02-11T12:44:52.109517Z","iopub.status.idle":"2025-02-11T12:44:52.221926Z","shell.execute_reply.started":"2025-02-11T12:44:52.109491Z","shell.execute_reply":"2025-02-11T12:44:52.221107Z"}},"outputs":[{"name":"stdout","text":"OOF 검증 데이터 roc-auc : 0.7402237231097757\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"submission['probability'] = oof_test_preds\nsubmission.to_csv('lgbm_submission.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T12:44:55.912158Z","iopub.execute_input":"2025-02-11T12:44:55.912453Z","iopub.status.idle":"2025-02-11T12:44:56.187913Z","shell.execute_reply.started":"2025-02-11T12:44:55.912430Z","shell.execute_reply":"2025-02-11T12:44:56.187263Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"submission = pd.read_csv('/kaggle/working/lgbm_submission.csv')\nsubmission.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T12:50:10.861744Z","iopub.execute_input":"2025-02-11T12:50:10.862123Z","iopub.status.idle":"2025-02-11T12:50:10.945177Z","shell.execute_reply.started":"2025-02-11T12:50:10.862073Z","shell.execute_reply":"2025-02-11T12:50:10.944325Z"}},"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"           ID  probability  임신 성공 여부\n0  TEST_00000     0.001947  0.001902\n1  TEST_00001     0.001889  0.001842\n2  TEST_00002     0.155794  0.155787\n3  TEST_00003     0.104613  0.104139\n4  TEST_00004     0.498481  0.497891","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>probability</th>\n      <th>임신 성공 여부</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TEST_00000</td>\n      <td>0.001947</td>\n      <td>0.001902</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TEST_00001</td>\n      <td>0.001889</td>\n      <td>0.001842</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TEST_00002</td>\n      <td>0.155794</td>\n      <td>0.155787</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>TEST_00003</td>\n      <td>0.104613</td>\n      <td>0.104139</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>TEST_00004</td>\n      <td>0.498481</td>\n      <td>0.497891</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"submission = submission.drop('임신 성공 여부', axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T12:52:30.881114Z","iopub.execute_input":"2025-02-11T12:52:30.881444Z","iopub.status.idle":"2025-02-11T12:52:30.887121Z","shell.execute_reply.started":"2025-02-11T12:52:30.881419Z","shell.execute_reply":"2025-02-11T12:52:30.886407Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"submission.to_csv('lgbm_submission.csv')\nsubmission.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T12:52:34.326922Z","iopub.execute_input":"2025-02-11T12:52:34.327254Z","iopub.status.idle":"2025-02-11T12:52:34.513005Z","shell.execute_reply.started":"2025-02-11T12:52:34.327227Z","shell.execute_reply":"2025-02-11T12:52:34.512183Z"}},"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"           ID  probability\n0  TEST_00000     0.001947\n1  TEST_00001     0.001889\n2  TEST_00002     0.155794\n3  TEST_00003     0.104613\n4  TEST_00004     0.498481","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>probability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TEST_00000</td>\n      <td>0.001947</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TEST_00001</td>\n      <td>0.001889</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TEST_00002</td>\n      <td>0.155794</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>TEST_00003</td>\n      <td>0.104613</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>TEST_00004</td>\n      <td>0.498481</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":51},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}