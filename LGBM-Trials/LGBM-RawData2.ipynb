{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 데이터셋 비교 (LGBM) - 베이스라인 버전","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-18T15:44:55.809174Z","iopub.execute_input":"2025-02-18T15:44:55.809504Z","iopub.status.idle":"2025-02-18T15:44:56.878605Z","shell.execute_reply.started":"2025-02-18T15:44:55.809472Z","shell.execute_reply":"2025-02-18T15:44:56.877946Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/lg-aimers/sample_submission.csv\n/kaggle/input/lg-aimers/train.csv\n/kaggle/input/lg-aimers/test.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"data_path = '/kaggle/input/lg-aimers/'\n\ntrain = pd.read_csv(data_path + 'train.csv', index_col= 'ID')\ntest = pd.read_csv(data_path + 'test.csv', index_col= 'ID')\nsubmission = pd.read_csv(data_path + 'sample_submission.csv', index_col= 'ID')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T15:44:59.019880Z","iopub.execute_input":"2025-02-18T15:44:59.020306Z","iopub.status.idle":"2025-02-18T15:45:02.828578Z","shell.execute_reply.started":"2025-02-18T15:44:59.020280Z","shell.execute_reply":"2025-02-18T15:45:02.827712Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## 1. 데이터 통합","metadata":{}},{"cell_type":"code","source":"all_data = pd.concat([train, test], ignore_index= True)\nall_data = all_data.drop('임신 성공 여부', axis= 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T15:45:06.172094Z","iopub.execute_input":"2025-02-18T15:45:06.172381Z","iopub.status.idle":"2025-02-18T15:45:06.358471Z","shell.execute_reply.started":"2025-02-18T15:45:06.172360Z","shell.execute_reply":"2025-02-18T15:45:06.357779Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## 2. 피처 엔지니어링","metadata":{}},{"cell_type":"code","source":"categorical_columns = [\n    \"시술 시기 코드\",\n    \"시술 당시 나이\",\n    \"시술 유형\",\n    \"특정 시술 유형\",\n    \"배란 자극 여부\",\n    \"배란 유도 유형\",\n    \"단일 배아 이식 여부\",\n    \"착상 전 유전 검사 사용 여부\",\n    \"착상 전 유전 진단 사용 여부\",\n    \"남성 주 불임 원인\",\n    \"남성 부 불임 원인\",\n    \"여성 주 불임 원인\",\n    \"여성 부 불임 원인\",\n    \"부부 주 불임 원인\",\n    \"부부 부 불임 원인\",\n    \"불명확 불임 원인\",\n    \"불임 원인 - 난관 질환\",\n    \"불임 원인 - 남성 요인\",\n    \"불임 원인 - 배란 장애\",\n    \"불임 원인 - 여성 요인\",\n    \"불임 원인 - 자궁경부 문제\",\n    \"불임 원인 - 자궁내막증\",\n    \"불임 원인 - 정자 농도\",\n    \"불임 원인 - 정자 면역학적 요인\",\n    \"불임 원인 - 정자 운동성\",\n    \"불임 원인 - 정자 형태\",\n    \"배아 생성 주요 이유\",\n    \"총 시술 횟수\",\n    \"클리닉 내 총 시술 횟수\",\n    \"IVF 시술 횟수\",\n    \"DI 시술 횟수\",\n    \"총 임신 횟수\",\n    \"IVF 임신 횟수\",\n    \"DI 임신 횟수\",\n    \"총 출산 횟수\",\n    \"IVF 출산 횟수\",\n    \"DI 출산 횟수\",\n    \"난자 출처\",\n    \"정자 출처\",\n    \"난자 기증자 나이\",\n    \"정자 기증자 나이\",\n    \"동결 배아 사용 여부\",\n    \"신선 배아 사용 여부\",\n    \"기증 배아 사용 여부\",\n    \"대리모 여부\",\n    \"PGD 시술 여부\",\n    \"PGS 시술 여부\"\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T15:45:08.458277Z","iopub.execute_input":"2025-02-18T15:45:08.458559Z","iopub.status.idle":"2025-02-18T15:45:08.463271Z","shell.execute_reply.started":"2025-02-18T15:45:08.458536Z","shell.execute_reply":"2025-02-18T15:45:08.462421Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# 카테고리형 컬럼들을 문자열로 변환\nfor col in categorical_columns:\n    all_data[col] = all_data[col].astype(str)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T15:45:10.739664Z","iopub.execute_input":"2025-02-18T15:45:10.740012Z","iopub.status.idle":"2025-02-18T15:45:13.481539Z","shell.execute_reply.started":"2025-02-18T15:45:10.739984Z","shell.execute_reply":"2025-02-18T15:45:13.480575Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from sklearn.preprocessing import  OrdinalEncoder\n\nordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n\nall_data_encoded = all_data.copy()\nall_data_encoded[categorical_columns] = ordinal_encoder.fit_transform(all_data[categorical_columns])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T15:45:13.482739Z","iopub.execute_input":"2025-02-18T15:45:13.483066Z","iopub.status.idle":"2025-02-18T15:45:19.612401Z","shell.execute_reply.started":"2025-02-18T15:45:13.483038Z","shell.execute_reply":"2025-02-18T15:45:19.611729Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"all_data_encoded[categorical_columns]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T16:57:53.735083Z","iopub.execute_input":"2025-02-18T16:57:53.735396Z","iopub.status.idle":"2025-02-18T16:57:54.129017Z","shell.execute_reply.started":"2025-02-18T16:57:53.735372Z","shell.execute_reply":"2025-02-18T16:57:54.128126Z"}},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"        시술 시기 코드  시술 당시 나이  시술 유형  특정 시술 유형  배란 자극 여부  배란 유도 유형  단일 배아 이식 여부  \\\n0            6.0       0.0    1.0       4.0       1.0       0.0          0.0   \n1            5.0       5.0    1.0       4.0       0.0       3.0          0.0   \n2            3.0       0.0    1.0      16.0       1.0       0.0          0.0   \n3            2.0       1.0    1.0       4.0       1.0       0.0          0.0   \n4            3.0       0.0    1.0       4.0       1.0       0.0          0.0   \n...          ...       ...    ...       ...       ...       ...          ...   \n346413       1.0       0.0    1.0      11.0       1.0       0.0          0.0   \n346414       5.0       4.0    1.0      25.0       0.0       3.0          0.0   \n346415       3.0       0.0    1.0      16.0       0.0       3.0          1.0   \n346416       0.0       4.0    1.0      25.0       0.0       3.0          0.0   \n346417       1.0       0.0    1.0      16.0       1.0       0.0          0.0   \n\n        착상 전 유전 검사 사용 여부  착상 전 유전 진단 사용 여부  남성 주 불임 원인  ...  난자 출처  정자 출처  \\\n0                    1.0               0.0         0.0  ...    1.0    3.0   \n1                    1.0               0.0         0.0  ...    1.0    3.0   \n2                    1.0               0.0         0.0  ...    1.0    3.0   \n3                    1.0               0.0         0.0  ...    1.0    3.0   \n4                    1.0               0.0         0.0  ...    1.0    3.0   \n...                  ...               ...         ...  ...    ...    ...   \n346413               1.0               1.0         0.0  ...    1.0    3.0   \n346414               1.0               0.0         0.0  ...    0.0    3.0   \n346415               1.0               0.0         0.0  ...    0.0    3.0   \n346416               1.0               0.0         0.0  ...    1.0    0.0   \n346417               1.0               0.0         0.0  ...    1.0    3.0   \n\n        난자 기증자 나이  정자 기증자 나이  동결 배아 사용 여부  신선 배아 사용 여부  기증 배아 사용 여부  대리모 여부  \\\n0             4.0        6.0          0.0          1.0          0.0     0.0   \n1             4.0        6.0          0.0          1.0          0.0     0.0   \n2             4.0        6.0          0.0          1.0          0.0     0.0   \n3             4.0        6.0          0.0          1.0          0.0     0.0   \n4             4.0        6.0          0.0          1.0          0.0     0.0   \n...           ...        ...          ...          ...          ...     ...   \n346413        4.0        6.0          0.0          1.0          0.0     0.0   \n346414        2.0        6.0          1.0          0.0          0.0     0.0   \n346415        1.0        6.0          0.0          1.0          0.0     0.0   \n346416        4.0        4.0          1.0          0.0          0.0     0.0   \n346417        4.0        6.0          0.0          1.0          0.0     0.0   \n\n        PGD 시술 여부  PGS 시술 여부  \n0             1.0        1.0  \n1             1.0        1.0  \n2             1.0        1.0  \n3             1.0        1.0  \n4             1.0        1.0  \n...           ...        ...  \n346413        1.0        1.0  \n346414        1.0        1.0  \n346415        1.0        1.0  \n346416        1.0        1.0  \n346417        1.0        1.0  \n\n[346418 rows x 47 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>시술 시기 코드</th>\n      <th>시술 당시 나이</th>\n      <th>시술 유형</th>\n      <th>특정 시술 유형</th>\n      <th>배란 자극 여부</th>\n      <th>배란 유도 유형</th>\n      <th>단일 배아 이식 여부</th>\n      <th>착상 전 유전 검사 사용 여부</th>\n      <th>착상 전 유전 진단 사용 여부</th>\n      <th>남성 주 불임 원인</th>\n      <th>...</th>\n      <th>난자 출처</th>\n      <th>정자 출처</th>\n      <th>난자 기증자 나이</th>\n      <th>정자 기증자 나이</th>\n      <th>동결 배아 사용 여부</th>\n      <th>신선 배아 사용 여부</th>\n      <th>기증 배아 사용 여부</th>\n      <th>대리모 여부</th>\n      <th>PGD 시술 여부</th>\n      <th>PGS 시술 여부</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>16.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>346413</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>11.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>346414</th>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>25.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>6.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>346415</th>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>16.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>346416</th>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>25.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>346417</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>16.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>346418 rows × 47 columns</p>\n</div>"},"metadata":{}}],"execution_count":40},{"cell_type":"code","source":"numeric_columns = [\n    \"임신 시도 또는 마지막 임신 경과 연수\",\n    \"총 생성 배아 수\",\n    \"미세주입된 난자 수\",\n    \"미세주입에서 생성된 배아 수\",\n    \"이식된 배아 수\",\n    \"미세주입 배아 이식 수\",\n    \"저장된 배아 수\",\n    \"미세주입 후 저장된 배아 수\",\n    \"해동된 배아 수\",\n    \"해동 난자 수\",\n    \"수집된 신선 난자 수\",\n    \"저장된 신선 난자 수\",\n    \"혼합된 난자 수\",\n    \"파트너 정자와 혼합된 난자 수\",\n    \"기증자 정자와 혼합된 난자 수\",\n    \"난자 채취 경과일\",\n    \"난자 해동 경과일\",\n    \"난자 혼합 경과일\",\n    \"배아 이식 경과일\",\n    \"배아 해동 경과일\"\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T15:45:21.707993Z","iopub.execute_input":"2025-02-18T15:45:21.708421Z","iopub.status.idle":"2025-02-18T15:45:21.712565Z","shell.execute_reply.started":"2025-02-18T15:45:21.708398Z","shell.execute_reply":"2025-02-18T15:45:21.711659Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"all_data_encoded[numeric_columns] = all_data_encoded[numeric_columns].fillna(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T15:45:21.914089Z","iopub.execute_input":"2025-02-18T15:45:21.914344Z","iopub.status.idle":"2025-02-18T15:45:22.021868Z","shell.execute_reply.started":"2025-02-18T15:45:21.914324Z","shell.execute_reply":"2025-02-18T15:45:22.021132Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## 3. 데이터 나누기","metadata":{}},{"cell_type":"code","source":"num_train = len(train) \n\nX = all_data_encoded[:num_train]\nX_test = all_data_encoded[num_train:]\n\ny = train['임신 성공 여부'].values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T15:45:26.286172Z","iopub.execute_input":"2025-02-18T15:45:26.286458Z","iopub.status.idle":"2025-02-18T15:45:26.290994Z","shell.execute_reply.started":"2025-02-18T15:45:26.286435Z","shell.execute_reply":"2025-02-18T15:45:26.290161Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## 4. 평가 함수 정의","metadata":{}},{"cell_type":"code","source":"def lgb_roc_auc(y_pred, dataset):\n    y_true = dataset.get_label()\n    return \"roc_auc\", roc_auc_score(y_true, y_pred), True  # (지표 이름, 값, 높은 값이 더 좋은지 여부)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T15:45:28.659939Z","iopub.execute_input":"2025-02-18T15:45:28.660228Z","iopub.status.idle":"2025-02-18T15:45:28.664227Z","shell.execute_reply.started":"2025-02-18T15:45:28.660208Z","shell.execute_reply":"2025-02-18T15:45:28.663282Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## 5. 파라미터 최적화","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\n\n# 8:2 비율로 훈련 데이터, 검증 데이터 분리 (베이지안 최적화 수행용)\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, \n                                                      test_size=0.2, \n                                                      random_state=0)\n\n# 베이지안 최적화용 데이터셋\nbayes_dtrain = lgb.Dataset(X_train, y_train)\nbayes_dvalid = lgb.Dataset(X_valid, y_valid)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T15:45:30.599012Z","iopub.execute_input":"2025-02-18T15:45:30.599397Z","iopub.status.idle":"2025-02-18T15:45:35.154921Z","shell.execute_reply.started":"2025-02-18T15:45:30.599354Z","shell.execute_reply":"2025-02-18T15:45:35.154219Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# 베이지안 최적화를 위한 하이퍼파라미터 범위\n# TODO: 점점 줄여나가자!!!!\nparam_bounds = {\n    'num_leaves': (30, 100),  \n    'lambda_l1': (0, 1),  \n    'lambda_l2': (0, 2),  \n    'feature_fraction': (0.7, 1),  \n    'bagging_fraction': (0.5, 0.8),  \n    'min_child_samples': (5, 50),  \n    'min_child_weight': (25, 50)  \n}\n\n# 값이 고정된 하이퍼파라미터\nfixed_params = {'objective': 'binary', # binary classification\n                'learning_rate': 0.005, # 0.01~0.001\n                'bagging_freq': 1, # 0 or 1\n                'force_row_wise': True,\n                'random_state': 1991}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T15:47:41.526657Z","iopub.execute_input":"2025-02-18T15:47:41.527458Z","iopub.status.idle":"2025-02-18T15:47:41.531980Z","shell.execute_reply.started":"2025-02-18T15:47:41.527425Z","shell.execute_reply":"2025-02-18T15:47:41.531137Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nfrom lightgbm import early_stopping\n\ndef eval_function(num_leaves, lambda_l1, lambda_l2, feature_fraction,\n                  bagging_fraction, min_child_samples, min_child_weight):\n    '''최적화하려는 평가지표 계산 함수'''\n    \n    # 베이지안 최적화를 수행할 하이퍼파라미터 \n    params = {'num_leaves': int(round(num_leaves)),\n              'lambda_l1': lambda_l1,\n              'lambda_l2': lambda_l2,\n              'feature_fraction': feature_fraction,\n              'bagging_fraction': bagging_fraction,\n              'min_child_samples': int(round(min_child_samples)),\n              'min_child_weight': min_child_weight,\n              'feature_pre_filter': False}\n    # 고정된 하이퍼파라미터도 추가\n    params.update(fixed_params)\n    \n    print('하이퍼파라미터:', params)    \n    \n    # LightGBM 모델 훈련\n    lgb_model = lgb.train(params=params, \n                           train_set=bayes_dtrain,\n                           num_boost_round=2500,\n                           valid_sets=bayes_dvalid,\n                           callbacks=[early_stopping(stopping_rounds=200)])\n    # 검증 데이터로 예측 수행\n    preds = lgb_model.predict(X_valid) \n    # roc-auc 계산\n    roc_auc = roc_auc_score(y_valid, preds)\n    print(f'roc-auc : {roc_auc}\\n')\n    \n    return roc_auc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T15:47:53.929620Z","iopub.execute_input":"2025-02-18T15:47:53.929937Z","iopub.status.idle":"2025-02-18T15:47:53.935836Z","shell.execute_reply.started":"2025-02-18T15:47:53.929913Z","shell.execute_reply":"2025-02-18T15:47:53.934951Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from bayes_opt import BayesianOptimization\n\n# 베이지안 최적화 객체 생성\noptimizer = BayesianOptimization(f=eval_function,      # 평가지표 계산 함수\n                                 pbounds=param_bounds, # 하이퍼파라미터 범위\n                                 random_state=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T15:48:03.675167Z","iopub.execute_input":"2025-02-18T15:48:03.675488Z","iopub.status.idle":"2025-02-18T15:48:03.732223Z","shell.execute_reply.started":"2025-02-18T15:48:03.675460Z","shell.execute_reply":"2025-02-18T15:48:03.731358Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# 베이지안 최적화 수행\n# TODO: init_points 10~15, n_iter 30~70\noptimizer.maximize(init_points=15, n_iter=60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T15:48:22.938850Z","iopub.execute_input":"2025-02-18T15:48:22.939159Z","iopub.status.idle":"2025-02-18T16:34:41.643492Z","shell.execute_reply.started":"2025-02-18T15:48:22.939136Z","shell.execute_reply":"2025-02-18T16:34:41.642696Z"}},"outputs":[{"name":"stdout","text":"|   iter    |  target   | baggin... | featur... | lambda_l1 | lambda_l2 | min_ch... | min_ch... | num_le... |\n-------------------------------------------------------------------------------------------------------------\n하이퍼파라미터: {'num_leaves': 61, 'lambda_l1': 0.6027633760716439, 'lambda_l2': 1.0897663659937937, 'feature_fraction': 0.9145568099117258, 'bagging_fraction': 0.6646440511781975, 'min_child_samples': 24, 'min_child_weight': 41.147352826666406, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1745]\tvalid_0's binary_logloss: 0.488831\nroc-auc : 0.7420036525633735\n\n| \u001b[39m1        \u001b[39m | \u001b[39m0.742    \u001b[39m | \u001b[39m0.6646   \u001b[39m | \u001b[39m0.9146   \u001b[39m | \u001b[39m0.6028   \u001b[39m | \u001b[39m1.09     \u001b[39m | \u001b[39m24.06    \u001b[39m | \u001b[39m41.15    \u001b[39m | \u001b[39m60.63    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 95, 'lambda_l1': 0.3834415188257777, 'lambda_l2': 1.5834500761653292, 'feature_fraction': 0.9890988281503088, 'bagging_fraction': 0.7675319002346239, 'min_child_samples': 29, 'min_child_weight': 39.20111402734831, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1305]\tvalid_0's binary_logloss: 0.489022\nroc-auc : 0.7417231833631315\n\n| \u001b[39m2        \u001b[39m | \u001b[39m0.7417   \u001b[39m | \u001b[39m0.7675   \u001b[39m | \u001b[39m0.9891   \u001b[39m | \u001b[39m0.3834   \u001b[39m | \u001b[39m1.583    \u001b[39m | \u001b[39m28.8     \u001b[39m | \u001b[39m39.2     \u001b[39m | \u001b[39m94.79    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 99, 'lambda_l1': 0.02021839744032572, 'lambda_l2': 1.665239691095876, 'feature_fraction': 0.7261387899104622, 'bagging_fraction': 0.5213108174593661, 'min_child_samples': 40, 'min_child_weight': 46.75030370617048, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1277]\tvalid_0's binary_logloss: 0.488939\nroc-auc : 0.7420414727280512\n\n| \u001b[35m3        \u001b[39m | \u001b[35m0.742    \u001b[39m | \u001b[35m0.5213   \u001b[39m | \u001b[35m0.7261   \u001b[39m | \u001b[35m0.02022  \u001b[39m | \u001b[35m1.665    \u001b[39m | \u001b[35m40.02    \u001b[39m | \u001b[35m46.75    \u001b[39m | \u001b[35m98.5     \u001b[39m |\n하이퍼파라미터: {'num_leaves': 96, 'lambda_l1': 0.7805291762864555, 'lambda_l2': 0.23654885173786644, 'feature_fraction': 0.8384438086758795, 'bagging_fraction': 0.7397475692650171, 'min_child_samples': 34, 'min_child_weight': 28.58383218522616, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1375]\tvalid_0's binary_logloss: 0.488946\nroc-auc : 0.7419013437515467\n\n| \u001b[39m4        \u001b[39m | \u001b[39m0.7419   \u001b[39m | \u001b[39m0.7397   \u001b[39m | \u001b[39m0.8384   \u001b[39m | \u001b[39m0.7805   \u001b[39m | \u001b[39m0.2365   \u001b[39m | \u001b[39m33.8     \u001b[39m | \u001b[39m28.58    \u001b[39m | \u001b[39m96.13    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 31, 'lambda_l1': 0.26455561210462697, 'lambda_l2': 1.5484673788684333, 'feature_fraction': 0.8243985819971571, 'bagging_fraction': 0.6565544965250215, 'min_child_samples': 26, 'min_child_weight': 39.21084872171621, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nDid not meet early stopping. Best iteration is:\n[2492]\tvalid_0's binary_logloss: 0.488683\nroc-auc : 0.7422425613383488\n\n| \u001b[35m5        \u001b[39m | \u001b[35m0.7422   \u001b[39m | \u001b[35m0.6566   \u001b[39m | \u001b[35m0.8244   \u001b[39m | \u001b[35m0.2646   \u001b[39m | \u001b[35m1.548    \u001b[39m | \u001b[35m25.53    \u001b[39m | \u001b[35m39.21    \u001b[39m | \u001b[35m31.32    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 61, 'lambda_l1': 0.6169339968747569, 'lambda_l2': 1.8874961570292483, 'feature_fraction': 0.8836287168167264, 'bagging_fraction': 0.6852906491227632, 'min_child_samples': 36, 'min_child_weight': 33.987697514344646, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1847]\tvalid_0's binary_logloss: 0.488795\nroc-auc : 0.7420730798549486\n\n| \u001b[39m6        \u001b[39m | \u001b[39m0.7421   \u001b[39m | \u001b[39m0.6853   \u001b[39m | \u001b[39m0.8836   \u001b[39m | \u001b[39m0.6169   \u001b[39m | \u001b[39m1.887    \u001b[39m | \u001b[39m35.68    \u001b[39m | \u001b[39m33.99    \u001b[39m | \u001b[39m60.59    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 52, 'lambda_l1': 0.6667667154456677, 'lambda_l2': 1.3412757392363188, 'feature_fraction': 0.7180676414887809, 'bagging_fraction': 0.7092893587781794, 'min_child_samples': 14, 'min_child_weight': 28.223157441371335, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[2007]\tvalid_0's binary_logloss: 0.488759\nroc-auc : 0.7421449443867968\n\n| \u001b[39m7        \u001b[39m | \u001b[39m0.7421   \u001b[39m | \u001b[39m0.7093   \u001b[39m | \u001b[39m0.7181   \u001b[39m | \u001b[39m0.6668   \u001b[39m | \u001b[39m1.341    \u001b[39m | \u001b[39m14.47    \u001b[39m | \u001b[39m28.22    \u001b[39m | \u001b[39m52.08    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 41, 'lambda_l1': 0.43860151346232035, 'lambda_l2': 1.9767476761184524, 'feature_fraction': 0.8710590311253639, 'bagging_fraction': 0.6091132312827868, 'min_child_samples': 10, 'min_child_weight': 30.221918902370867, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[2173]\tvalid_0's binary_logloss: 0.488724\nroc-auc : 0.7421518336131503\n\n| \u001b[39m8        \u001b[39m | \u001b[39m0.7422   \u001b[39m | \u001b[39m0.6091   \u001b[39m | \u001b[39m0.8711   \u001b[39m | \u001b[39m0.4386   \u001b[39m | \u001b[39m1.977    \u001b[39m | \u001b[39m9.592    \u001b[39m | \u001b[39m30.22    \u001b[39m | \u001b[39m41.29    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 76, 'lambda_l1': 0.4663107728563063, 'lambda_l2': 0.4888511840032055, 'feature_fraction': 0.7759874807619346, 'bagging_fraction': 0.6959324976396195, 'min_child_samples': 12, 'min_child_weight': 27.75937852910763, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1538]\tvalid_0's binary_logloss: 0.488876\nroc-auc : 0.7419879383357876\n\n| \u001b[39m9        \u001b[39m | \u001b[39m0.742    \u001b[39m | \u001b[39m0.6959   \u001b[39m | \u001b[39m0.776    \u001b[39m | \u001b[39m0.4663   \u001b[39m | \u001b[39m0.4889   \u001b[39m | \u001b[39m12.15    \u001b[39m | \u001b[39m27.76    \u001b[39m | \u001b[39m75.94    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 37, 'lambda_l1': 0.3687251706609641, 'lambda_l2': 1.6419864596958702, 'feature_fraction': 0.758974708504016, 'bagging_fraction': 0.5414548854045842, 'min_child_samples': 9, 'min_child_weight': 45.9486226874701, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[2199]\tvalid_0's binary_logloss: 0.488755\nroc-auc : 0.7421926714544239\n\n| \u001b[39m10       \u001b[39m | \u001b[39m0.7422   \u001b[39m | \u001b[39m0.5415   \u001b[39m | \u001b[39m0.759    \u001b[39m | \u001b[39m0.3687   \u001b[39m | \u001b[39m1.642    \u001b[39m | \u001b[39m9.37     \u001b[39m | \u001b[39m45.95    \u001b[39m | \u001b[39m36.73    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 50, 'lambda_l1': 0.9767610881903371, 'lambda_l2': 1.209691039490092, 'feature_fraction': 0.8405953604943104, 'bagging_fraction': 0.7929378395040187, 'min_child_samples': 38, 'min_child_weight': 25.979694806358015, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[2097]\tvalid_0's binary_logloss: 0.48889\nroc-auc : 0.7418582542516813\n\n| \u001b[39m11       \u001b[39m | \u001b[39m0.7419   \u001b[39m | \u001b[39m0.7929   \u001b[39m | \u001b[39m0.8406   \u001b[39m | \u001b[39m0.9768   \u001b[39m | \u001b[39m1.21     \u001b[39m | \u001b[39m38.27    \u001b[39m | \u001b[39m25.98    \u001b[39m | \u001b[39m49.8     \u001b[39m |\n하이퍼파라미터: {'num_leaves': 78, 'lambda_l1': 0.11872771895424405, 'lambda_l2': 0.635966358787952, 'feature_fraction': 0.7888420592566434, 'bagging_fraction': 0.5360589683639507, 'min_child_samples': 24, 'min_child_weight': 26.60368740871961, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1544]\tvalid_0's binary_logloss: 0.488827\nroc-auc : 0.7420625095962018\n\n| \u001b[39m12       \u001b[39m | \u001b[39m0.7421   \u001b[39m | \u001b[39m0.5361   \u001b[39m | \u001b[39m0.7888   \u001b[39m | \u001b[39m0.1187   \u001b[39m | \u001b[39m0.636    \u001b[39m | \u001b[39m23.64    \u001b[39m | \u001b[39m26.6     \u001b[39m | \u001b[39m78.47    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 52, 'lambda_l1': 0.5232480534666997, 'lambda_l2': 0.18788102151688335, 'feature_fraction': 0.7796168472818336, 'bagging_fraction': 0.6699804362619726, 'min_child_samples': 31, 'min_child_weight': 48.232404939405356, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1907]\tvalid_0's binary_logloss: 0.488813\nroc-auc : 0.742072613926472\n\n| \u001b[39m13       \u001b[39m | \u001b[39m0.7421   \u001b[39m | \u001b[39m0.67     \u001b[39m | \u001b[39m0.7796   \u001b[39m | \u001b[39m0.5232   \u001b[39m | \u001b[39m0.1879   \u001b[39m | \u001b[39m30.92    \u001b[39m | \u001b[39m48.23    \u001b[39m | \u001b[39m52.3     \u001b[39m |\n하이퍼파라미터: {'num_leaves': 31, 'lambda_l1': 0.7163272041185655, 'lambda_l2': 0.5788121858944022, 'feature_fraction': 0.7395393587213176, 'bagging_fraction': 0.7002231139891045, 'min_child_samples': 13, 'min_child_weight': 39.66282337025208, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nDid not meet early stopping. Best iteration is:\n[2478]\tvalid_0's binary_logloss: 0.488701\nroc-auc : 0.7422236695180371\n\n| \u001b[39m14       \u001b[39m | \u001b[39m0.7422   \u001b[39m | \u001b[39m0.7002   \u001b[39m | \u001b[39m0.7395   \u001b[39m | \u001b[39m0.7163   \u001b[39m | \u001b[39m0.5788   \u001b[39m | \u001b[39m13.24    \u001b[39m | \u001b[39m39.66    \u001b[39m | \u001b[39m31.41    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 47, 'lambda_l1': 0.6778165367962301, 'lambda_l2': 0.5400159463843297, 'feature_fraction': 0.701408642857764, 'bagging_fraction': 0.748682008765209, 'min_child_samples': 38, 'min_child_weight': 49.05471362793595, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[2242]\tvalid_0's binary_logloss: 0.488782\nroc-auc : 0.7421172463208481\n\n| \u001b[39m15       \u001b[39m | \u001b[39m0.7421   \u001b[39m | \u001b[39m0.7487   \u001b[39m | \u001b[39m0.7014   \u001b[39m | \u001b[39m0.6778   \u001b[39m | \u001b[39m0.54     \u001b[39m | \u001b[39m38.08    \u001b[39m | \u001b[39m49.05    \u001b[39m | \u001b[39m47.41    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 30, 'lambda_l1': 0.5386784890128765, 'lambda_l2': 0.609611896765758, 'feature_fraction': 0.7927624479811346, 'bagging_fraction': 0.5575571715785211, 'min_child_samples': 23, 'min_child_weight': 49.91431377990331, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nDid not meet early stopping. Best iteration is:\n[2301]\tvalid_0's binary_logloss: 0.488801\nroc-auc : 0.7421148347461265\n\n| \u001b[39m16       \u001b[39m | \u001b[39m0.7421   \u001b[39m | \u001b[39m0.5576   \u001b[39m | \u001b[39m0.7928   \u001b[39m | \u001b[39m0.5387   \u001b[39m | \u001b[39m0.6096   \u001b[39m | \u001b[39m22.89    \u001b[39m | \u001b[39m49.91    \u001b[39m | \u001b[39m30.42    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 30, 'lambda_l1': 0.5719872232960876, 'lambda_l2': 1.3121929442066727, 'feature_fraction': 0.769568853739818, 'bagging_fraction': 0.7587921297443723, 'min_child_samples': 22, 'min_child_weight': 28.434769797693995, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nDid not meet early stopping. Best iteration is:\n[2495]\tvalid_0's binary_logloss: 0.488802\nroc-auc : 0.7420192394503379\n\n| \u001b[39m17       \u001b[39m | \u001b[39m0.742    \u001b[39m | \u001b[39m0.7588   \u001b[39m | \u001b[39m0.7696   \u001b[39m | \u001b[39m0.572    \u001b[39m | \u001b[39m1.312    \u001b[39m | \u001b[39m21.63    \u001b[39m | \u001b[39m28.43    \u001b[39m | \u001b[39m30.46    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 32, 'lambda_l1': 0.4038318027662431, 'lambda_l2': 1.31908281002761, 'feature_fraction': 0.9597097106367154, 'bagging_fraction': 0.7389139361765316, 'min_child_samples': 26, 'min_child_weight': 38.57058368700699, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nDid not meet early stopping. Best iteration is:\n[2495]\tvalid_0's binary_logloss: 0.488757\nroc-auc : 0.742051558302726\n\n| \u001b[39m18       \u001b[39m | \u001b[39m0.7421   \u001b[39m | \u001b[39m0.7389   \u001b[39m | \u001b[39m0.9597   \u001b[39m | \u001b[39m0.4038   \u001b[39m | \u001b[39m1.319    \u001b[39m | \u001b[39m26.45    \u001b[39m | \u001b[39m38.57    \u001b[39m | \u001b[39m31.72    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 75, 'lambda_l1': 0.8104622511094248, 'lambda_l2': 0.010229761950819816, 'feature_fraction': 0.8668742306742858, 'bagging_fraction': 0.6236613926476847, 'min_child_samples': 30, 'min_child_weight': 30.667497952753696, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1433]\tvalid_0's binary_logloss: 0.48886\nroc-auc : 0.7420068824743392\n\n| \u001b[39m19       \u001b[39m | \u001b[39m0.742    \u001b[39m | \u001b[39m0.6237   \u001b[39m | \u001b[39m0.8669   \u001b[39m | \u001b[39m0.8105   \u001b[39m | \u001b[39m0.01023  \u001b[39m | \u001b[39m30.01    \u001b[39m | \u001b[39m30.67    \u001b[39m | \u001b[39m74.65    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 44, 'lambda_l1': 0.6496633125507695, 'lambda_l2': 0.6187645444134686, 'feature_fraction': 0.9856582162480627, 'bagging_fraction': 0.5675659367709145, 'min_child_samples': 26, 'min_child_weight': 40.50339078393357, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1656]\tvalid_0's binary_logloss: 0.488772\nroc-auc : 0.7421384105296203\n\n| \u001b[39m20       \u001b[39m | \u001b[39m0.7421   \u001b[39m | \u001b[39m0.5676   \u001b[39m | \u001b[39m0.9857   \u001b[39m | \u001b[39m0.6497   \u001b[39m | \u001b[39m0.6188   \u001b[39m | \u001b[39m26.15    \u001b[39m | \u001b[39m40.5     \u001b[39m | \u001b[39m43.56    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 52, 'lambda_l1': 0.48597576105191065, 'lambda_l2': 0.028821416460550475, 'feature_fraction': 0.7107210455606096, 'bagging_fraction': 0.7183998010416465, 'min_child_samples': 31, 'min_child_weight': 48.61332950591831, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1939]\tvalid_0's binary_logloss: 0.488803\nroc-auc : 0.7421092110288983\n\n| \u001b[39m21       \u001b[39m | \u001b[39m0.7421   \u001b[39m | \u001b[39m0.7184   \u001b[39m | \u001b[39m0.7107   \u001b[39m | \u001b[39m0.486    \u001b[39m | \u001b[39m0.02882  \u001b[39m | \u001b[39m31.28    \u001b[39m | \u001b[39m48.61    \u001b[39m | \u001b[39m52.46    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 32, 'lambda_l1': 0.010830476367927666, 'lambda_l2': 1.1130118566703664, 'feature_fraction': 0.8578787214074411, 'bagging_fraction': 0.7612184808150626, 'min_child_samples': 13, 'min_child_weight': 39.22897397130396, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[2162]\tvalid_0's binary_logloss: 0.488821\nroc-auc : 0.7419926992956247\n\n| \u001b[39m22       \u001b[39m | \u001b[39m0.742    \u001b[39m | \u001b[39m0.7612   \u001b[39m | \u001b[39m0.8579   \u001b[39m | \u001b[39m0.01083  \u001b[39m | \u001b[39m1.113    \u001b[39m | \u001b[39m13.34    \u001b[39m | \u001b[39m39.23    \u001b[39m | \u001b[39m31.57    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 42, 'lambda_l1': 0.29011729730162206, 'lambda_l2': 1.8925572652728049, 'feature_fraction': 0.9323014481336473, 'bagging_fraction': 0.76140417989194, 'min_child_samples': 9, 'min_child_weight': 30.181613248603618, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[2160]\tvalid_0's binary_logloss: 0.488893\nroc-auc : 0.7418124718431623\n\n| \u001b[39m23       \u001b[39m | \u001b[39m0.7418   \u001b[39m | \u001b[39m0.7614   \u001b[39m | \u001b[39m0.9323   \u001b[39m | \u001b[39m0.2901   \u001b[39m | \u001b[39m1.893    \u001b[39m | \u001b[39m9.18     \u001b[39m | \u001b[39m30.18    \u001b[39m | \u001b[39m41.59    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 94, 'lambda_l1': 0.7763830164811878, 'lambda_l2': 0.7496182449888358, 'feature_fraction': 0.7846702039322525, 'bagging_fraction': 0.7306374348160593, 'min_child_samples': 29, 'min_child_weight': 33.725438090183346, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1295]\tvalid_0's binary_logloss: 0.488972\nroc-auc : 0.7418735519076207\n\n| \u001b[39m24       \u001b[39m | \u001b[39m0.7419   \u001b[39m | \u001b[39m0.7306   \u001b[39m | \u001b[39m0.7847   \u001b[39m | \u001b[39m0.7764   \u001b[39m | \u001b[39m0.7496   \u001b[39m | \u001b[39m29.43    \u001b[39m | \u001b[39m33.73    \u001b[39m | \u001b[39m94.1     \u001b[39m |\n하이퍼파라미터: {'num_leaves': 76, 'lambda_l1': 0.8449802394703055, 'lambda_l2': 1.8725120754477054, 'feature_fraction': 0.7674476962109258, 'bagging_fraction': 0.5508242096580529, 'min_child_samples': 43, 'min_child_weight': 43.508560145436775, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1554]\tvalid_0's binary_logloss: 0.488886\nroc-auc : 0.7420344887365837\n\n| \u001b[39m25       \u001b[39m | \u001b[39m0.742    \u001b[39m | \u001b[39m0.5508   \u001b[39m | \u001b[39m0.7674   \u001b[39m | \u001b[39m0.845    \u001b[39m | \u001b[39m1.873    \u001b[39m | \u001b[39m43.03    \u001b[39m | \u001b[39m43.51    \u001b[39m | \u001b[39m75.74    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 88, 'lambda_l1': 0.4436875344350202, 'lambda_l2': 0.9620968144016055, 'feature_fraction': 0.8169234182751323, 'bagging_fraction': 0.5223702388453385, 'min_child_samples': 44, 'min_child_weight': 38.12350257664967, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1355]\tvalid_0's binary_logloss: 0.488876\nroc-auc : 0.7420593053507878\n\n| \u001b[39m26       \u001b[39m | \u001b[39m0.7421   \u001b[39m | \u001b[39m0.5224   \u001b[39m | \u001b[39m0.8169   \u001b[39m | \u001b[39m0.4437   \u001b[39m | \u001b[39m0.9621   \u001b[39m | \u001b[39m43.96    \u001b[39m | \u001b[39m38.12    \u001b[39m | \u001b[39m87.67    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 45, 'lambda_l1': 0.39033878005837175, 'lambda_l2': 0.2923495261467115, 'feature_fraction': 0.7289597278200716, 'bagging_fraction': 0.6045248539201262, 'min_child_samples': 26, 'min_child_weight': 30.501817154028583, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[2207]\tvalid_0's binary_logloss: 0.488679\nroc-auc : 0.7422697470804003\n\n| \u001b[35m27       \u001b[39m | \u001b[35m0.7423   \u001b[39m | \u001b[35m0.6045   \u001b[39m | \u001b[35m0.729    \u001b[39m | \u001b[35m0.3903   \u001b[39m | \u001b[35m0.2923   \u001b[39m | \u001b[35m25.62    \u001b[39m | \u001b[35m30.5     \u001b[39m | \u001b[35m44.55    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 80, 'lambda_l1': 0.7212025476379865, 'lambda_l2': 0.9939000440509653, 'feature_fraction': 0.8737693328089058, 'bagging_fraction': 0.6680968226265601, 'min_child_samples': 38, 'min_child_weight': 46.17681818701517, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1613]\tvalid_0's binary_logloss: 0.488914\nroc-auc : 0.7419363298470367\n\n| \u001b[39m28       \u001b[39m | \u001b[39m0.7419   \u001b[39m | \u001b[39m0.6681   \u001b[39m | \u001b[39m0.8738   \u001b[39m | \u001b[39m0.7212   \u001b[39m | \u001b[39m0.9939   \u001b[39m | \u001b[39m38.2     \u001b[39m | \u001b[39m46.18    \u001b[39m | \u001b[39m79.62    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 81, 'lambda_l1': 0.6496474667289607, 'lambda_l2': 0.9737204494517451, 'feature_fraction': 0.8023987891250263, 'bagging_fraction': 0.7857521251816739, 'min_child_samples': 6, 'min_child_weight': 34.10621699160177, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1521]\tvalid_0's binary_logloss: 0.488945\nroc-auc : 0.7418070228491128\n\n| \u001b[39m29       \u001b[39m | \u001b[39m0.7418   \u001b[39m | \u001b[39m0.7858   \u001b[39m | \u001b[39m0.8024   \u001b[39m | \u001b[39m0.6496   \u001b[39m | \u001b[39m0.9737   \u001b[39m | \u001b[39m6.309    \u001b[39m | \u001b[39m34.11    \u001b[39m | \u001b[39m81.09    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 83, 'lambda_l1': 0.661657417846386, 'lambda_l2': 0.17908143013052746, 'feature_fraction': 0.8198291515804634, 'bagging_fraction': 0.6130658099003511, 'min_child_samples': 8, 'min_child_weight': 37.53014149442329, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1446]\tvalid_0's binary_logloss: 0.488812\nroc-auc : 0.7421286753884397\n\n| \u001b[39m30       \u001b[39m | \u001b[39m0.7421   \u001b[39m | \u001b[39m0.6131   \u001b[39m | \u001b[39m0.8198   \u001b[39m | \u001b[39m0.6617   \u001b[39m | \u001b[39m0.1791   \u001b[39m | \u001b[39m7.961    \u001b[39m | \u001b[39m37.53    \u001b[39m | \u001b[39m83.02    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 57, 'lambda_l1': 0.8479306688699249, 'lambda_l2': 0.358052383512776, 'feature_fraction': 0.9550620369642995, 'bagging_fraction': 0.5766453372627729, 'min_child_samples': 19, 'min_child_weight': 48.34665911562921, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1631]\tvalid_0's binary_logloss: 0.488829\nroc-auc : 0.7420716445583275\n\n| \u001b[39m31       \u001b[39m | \u001b[39m0.7421   \u001b[39m | \u001b[39m0.5766   \u001b[39m | \u001b[39m0.9551   \u001b[39m | \u001b[39m0.8479   \u001b[39m | \u001b[39m0.3581   \u001b[39m | \u001b[39m19.28    \u001b[39m | \u001b[39m48.35    \u001b[39m | \u001b[39m56.51    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 94, 'lambda_l1': 0.3174532162959055, 'lambda_l2': 1.2973010638477942, 'feature_fraction': 0.87115146351822, 'bagging_fraction': 0.5137564943052509, 'min_child_samples': 15, 'min_child_weight': 30.12736493238523, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1449]\tvalid_0's binary_logloss: 0.488857\nroc-auc : 0.741993305397499\n\n| \u001b[39m32       \u001b[39m | \u001b[39m0.742    \u001b[39m | \u001b[39m0.5138   \u001b[39m | \u001b[39m0.8712   \u001b[39m | \u001b[39m0.3175   \u001b[39m | \u001b[39m1.297    \u001b[39m | \u001b[39m15.05    \u001b[39m | \u001b[39m30.13    \u001b[39m | \u001b[39m94.31    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 91, 'lambda_l1': 0.30126036764920805, 'lambda_l2': 0.4855919263933808, 'feature_fraction': 0.8269656037696734, 'bagging_fraction': 0.6934313354834455, 'min_child_samples': 21, 'min_child_weight': 43.63596576111247, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1420]\tvalid_0's binary_logloss: 0.48888\nroc-auc : 0.7420108152265663\n\n| \u001b[39m33       \u001b[39m | \u001b[39m0.742    \u001b[39m | \u001b[39m0.6934   \u001b[39m | \u001b[39m0.827    \u001b[39m | \u001b[39m0.3013   \u001b[39m | \u001b[39m0.4856   \u001b[39m | \u001b[39m20.5     \u001b[39m | \u001b[39m43.64    \u001b[39m | \u001b[39m91.09    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 96, 'lambda_l1': 0.012490002211076545, 'lambda_l2': 0.07974391556731142, 'feature_fraction': 0.7485370911513198, 'bagging_fraction': 0.5076207327956567, 'min_child_samples': 17, 'min_child_weight': 25.405684052638712, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1290]\tvalid_0's binary_logloss: 0.48889\nroc-auc : 0.7420261898791609\n\n| \u001b[39m34       \u001b[39m | \u001b[39m0.742    \u001b[39m | \u001b[39m0.5076   \u001b[39m | \u001b[39m0.7485   \u001b[39m | \u001b[39m0.01249  \u001b[39m | \u001b[39m0.07974  \u001b[39m | \u001b[39m17.3     \u001b[39m | \u001b[39m25.41    \u001b[39m | \u001b[39m96.1     \u001b[39m |\n하이퍼파라미터: {'num_leaves': 67, 'lambda_l1': 0.14804084224057346, 'lambda_l2': 1.8766799266352552, 'feature_fraction': 0.7298181924451745, 'bagging_fraction': 0.6327676947850891, 'min_child_samples': 9, 'min_child_weight': 40.38117795850751, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1637]\tvalid_0's binary_logloss: 0.488802\nroc-auc : 0.7421669920828289\n\n| \u001b[39m35       \u001b[39m | \u001b[39m0.7422   \u001b[39m | \u001b[39m0.6328   \u001b[39m | \u001b[39m0.7298   \u001b[39m | \u001b[39m0.148    \u001b[39m | \u001b[39m1.877    \u001b[39m | \u001b[39m9.208    \u001b[39m | \u001b[39m40.38    \u001b[39m | \u001b[39m66.52    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 52, 'lambda_l1': 0.40353710655593267, 'lambda_l2': 0.2996485713602981, 'feature_fraction': 0.938223780895125, 'bagging_fraction': 0.6795251778233339, 'min_child_samples': 28, 'min_child_weight': 37.17585110473753, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1945]\tvalid_0's binary_logloss: 0.488793\nroc-auc : 0.7420479730225833\n\n| \u001b[39m36       \u001b[39m | \u001b[39m0.742    \u001b[39m | \u001b[39m0.6795   \u001b[39m | \u001b[39m0.9382   \u001b[39m | \u001b[39m0.4035   \u001b[39m | \u001b[39m0.2996   \u001b[39m | \u001b[39m28.09    \u001b[39m | \u001b[39m37.18    \u001b[39m | \u001b[39m52.23    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 82, 'lambda_l1': 0.11191229446387174, 'lambda_l2': 1.091412502884368, 'feature_fraction': 0.8288956576929231, 'bagging_fraction': 0.5505918074978806, 'min_child_samples': 6, 'min_child_weight': 37.84187927804233, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1495]\tvalid_0's binary_logloss: 0.488896\nroc-auc : 0.7420290890993644\n\n| \u001b[39m37       \u001b[39m | \u001b[39m0.742    \u001b[39m | \u001b[39m0.5506   \u001b[39m | \u001b[39m0.8289   \u001b[39m | \u001b[39m0.1119   \u001b[39m | \u001b[39m1.091    \u001b[39m | \u001b[39m5.989    \u001b[39m | \u001b[39m37.84    \u001b[39m | \u001b[39m82.2     \u001b[39m |\n하이퍼파라미터: {'num_leaves': 99, 'lambda_l1': 0.0016264112300962141, 'lambda_l2': 0.9991373923844269, 'feature_fraction': 0.9451023972973045, 'bagging_fraction': 0.5604256492906088, 'min_child_samples': 28, 'min_child_weight': 49.81800870993711, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1246]\tvalid_0's binary_logloss: 0.489002\nroc-auc : 0.7418175062398386\n\n| \u001b[39m38       \u001b[39m | \u001b[39m0.7418   \u001b[39m | \u001b[39m0.5604   \u001b[39m | \u001b[39m0.9451   \u001b[39m | \u001b[39m0.001626 \u001b[39m | \u001b[39m0.9991   \u001b[39m | \u001b[39m28.35    \u001b[39m | \u001b[39m49.82    \u001b[39m | \u001b[39m99.03    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 97, 'lambda_l1': 0.2704317924988554, 'lambda_l2': 1.9952199002460516, 'feature_fraction': 0.9422645450545837, 'bagging_fraction': 0.6567134597217293, 'min_child_samples': 24, 'min_child_weight': 27.016016787264018, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1256]\tvalid_0's binary_logloss: 0.48893\nroc-auc : 0.7418580400430383\n\n| \u001b[39m39       \u001b[39m | \u001b[39m0.7419   \u001b[39m | \u001b[39m0.6567   \u001b[39m | \u001b[39m0.9423   \u001b[39m | \u001b[39m0.2704   \u001b[39m | \u001b[39m1.995    \u001b[39m | \u001b[39m24.42    \u001b[39m | \u001b[39m27.02    \u001b[39m | \u001b[39m97.09    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 70, 'lambda_l1': 0.9031980750563094, 'lambda_l2': 1.6898634995580846, 'feature_fraction': 0.9778148436372402, 'bagging_fraction': 0.5499986120062463, 'min_child_samples': 38, 'min_child_weight': 32.58867013085979, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1437]\tvalid_0's binary_logloss: 0.488803\nroc-auc : 0.7420898730228419\n\n| \u001b[39m40       \u001b[39m | \u001b[39m0.7421   \u001b[39m | \u001b[39m0.55     \u001b[39m | \u001b[39m0.9778   \u001b[39m | \u001b[39m0.9032   \u001b[39m | \u001b[39m1.69     \u001b[39m | \u001b[39m38.33    \u001b[39m | \u001b[39m32.59    \u001b[39m | \u001b[39m70.31    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 72, 'lambda_l1': 0.6281355378268801, 'lambda_l2': 0.853510422148881, 'feature_fraction': 0.803737301333092, 'bagging_fraction': 0.721702560738939, 'min_child_samples': 15, 'min_child_weight': 30.071091933997987, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1444]\tvalid_0's binary_logloss: 0.488866\nroc-auc : 0.7419995677921096\n\n| \u001b[39m41       \u001b[39m | \u001b[39m0.742    \u001b[39m | \u001b[39m0.7217   \u001b[39m | \u001b[39m0.8037   \u001b[39m | \u001b[39m0.6281   \u001b[39m | \u001b[39m0.8535   \u001b[39m | \u001b[39m14.81    \u001b[39m | \u001b[39m30.07    \u001b[39m | \u001b[39m72.29    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 95, 'lambda_l1': 0.8348405724898013, 'lambda_l2': 0.24803322361439717, 'feature_fraction': 0.9434416832468362, 'bagging_fraction': 0.7403157772429224, 'min_child_samples': 42, 'min_child_weight': 29.83363427409915, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1301]\tvalid_0's binary_logloss: 0.48899\nroc-auc : 0.7417689312218692\n\n| \u001b[39m42       \u001b[39m | \u001b[39m0.7418   \u001b[39m | \u001b[39m0.7403   \u001b[39m | \u001b[39m0.9434   \u001b[39m | \u001b[39m0.8348   \u001b[39m | \u001b[39m0.248    \u001b[39m | \u001b[39m41.54    \u001b[39m | \u001b[39m29.83    \u001b[39m | \u001b[39m94.86    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 75, 'lambda_l1': 0.09353414167709528, 'lambda_l2': 1.5955734559999646, 'feature_fraction': 0.8665081229601407, 'bagging_fraction': 0.6602973168599747, 'min_child_samples': 45, 'min_child_weight': 30.238491719761154, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1364]\tvalid_0's binary_logloss: 0.488856\nroc-auc : 0.7420073049688055\n\n| \u001b[39m43       \u001b[39m | \u001b[39m0.742    \u001b[39m | \u001b[39m0.6603   \u001b[39m | \u001b[39m0.8665   \u001b[39m | \u001b[39m0.09353  \u001b[39m | \u001b[39m1.596    \u001b[39m | \u001b[39m45.41    \u001b[39m | \u001b[39m30.24    \u001b[39m | \u001b[39m74.67    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 91, 'lambda_l1': 0.4217239280793025, 'lambda_l2': 1.7809302994842524, 'feature_fraction': 0.8810366813557409, 'bagging_fraction': 0.5865888828820108, 'min_child_samples': 48, 'min_child_weight': 47.2330458588856, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1447]\tvalid_0's binary_logloss: 0.488876\nroc-auc : 0.7420610170456577\n\n| \u001b[39m44       \u001b[39m | \u001b[39m0.7421   \u001b[39m | \u001b[39m0.5866   \u001b[39m | \u001b[39m0.881    \u001b[39m | \u001b[39m0.4217   \u001b[39m | \u001b[39m1.781    \u001b[39m | \u001b[39m48.3     \u001b[39m | \u001b[39m47.23    \u001b[39m | \u001b[39m91.12    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 61, 'lambda_l1': 0.480208963174523, 'lambda_l2': 1.412304256083041, 'feature_fraction': 0.9362053094923584, 'bagging_fraction': 0.6465609721488106, 'min_child_samples': 38, 'min_child_weight': 38.50981672256064, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1807]\tvalid_0's binary_logloss: 0.488771\nroc-auc : 0.742110930620861\n\n| \u001b[39m45       \u001b[39m | \u001b[39m0.7421   \u001b[39m | \u001b[39m0.6466   \u001b[39m | \u001b[39m0.9362   \u001b[39m | \u001b[39m0.4802   \u001b[39m | \u001b[39m1.412    \u001b[39m | \u001b[39m38.35    \u001b[39m | \u001b[39m38.51    \u001b[39m | \u001b[39m60.94    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 96, 'lambda_l1': 0.40953290273243503, 'lambda_l2': 1.5259258688995812, 'feature_fraction': 0.8693118247015201, 'bagging_fraction': 0.5162433142917655, 'min_child_samples': 29, 'min_child_weight': 43.29082818203824, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1435]\tvalid_0's binary_logloss: 0.488969\nroc-auc : 0.7419106800895396\n\n| \u001b[39m46       \u001b[39m | \u001b[39m0.7419   \u001b[39m | \u001b[39m0.5162   \u001b[39m | \u001b[39m0.8693   \u001b[39m | \u001b[39m0.4095   \u001b[39m | \u001b[39m1.526    \u001b[39m | \u001b[39m29.41    \u001b[39m | \u001b[39m43.29    \u001b[39m | \u001b[39m96.39    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 36, 'lambda_l1': 0.27629142068769685, 'lambda_l2': 0.4712228966008998, 'feature_fraction': 0.8681399435410332, 'bagging_fraction': 0.5740665177925204, 'min_child_samples': 40, 'min_child_weight': 49.54045683701298, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[2201]\tvalid_0's binary_logloss: 0.48872\nroc-auc : 0.7422750953865165\n\n| \u001b[35m47       \u001b[39m | \u001b[35m0.7423   \u001b[39m | \u001b[35m0.5741   \u001b[39m | \u001b[35m0.8681   \u001b[39m | \u001b[35m0.2763   \u001b[39m | \u001b[35m0.4712   \u001b[39m | \u001b[35m39.69    \u001b[39m | \u001b[35m49.54    \u001b[39m | \u001b[35m36.31    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 63, 'lambda_l1': 0.9106326218932754, 'lambda_l2': 1.5413349113577637, 'feature_fraction': 0.9237958912810729, 'bagging_fraction': 0.7051735572048502, 'min_child_samples': 8, 'min_child_weight': 25.057873639794366, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1627]\tvalid_0's binary_logloss: 0.488864\nroc-auc : 0.7419087512246169\n\n| \u001b[39m48       \u001b[39m | \u001b[39m0.7419   \u001b[39m | \u001b[39m0.7052   \u001b[39m | \u001b[39m0.9238   \u001b[39m | \u001b[39m0.9106   \u001b[39m | \u001b[39m1.541    \u001b[39m | \u001b[39m8.265    \u001b[39m | \u001b[39m25.06    \u001b[39m | \u001b[39m63.42    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 55, 'lambda_l1': 0.45186398315793674, 'lambda_l2': 0.10541459309859325, 'feature_fraction': 0.9274883912244153, 'bagging_fraction': 0.6855082957597972, 'min_child_samples': 43, 'min_child_weight': 27.322196738479228, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1748]\tvalid_0's binary_logloss: 0.488814\nroc-auc : 0.7419791932926201\n\n| \u001b[39m49       \u001b[39m | \u001b[39m0.742    \u001b[39m | \u001b[39m0.6855   \u001b[39m | \u001b[39m0.9275   \u001b[39m | \u001b[39m0.4519   \u001b[39m | \u001b[39m0.1054   \u001b[39m | \u001b[39m42.9     \u001b[39m | \u001b[39m27.32    \u001b[39m | \u001b[39m54.68    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 72, 'lambda_l1': 0.9831138281261637, 'lambda_l2': 1.4804529082565345, 'feature_fraction': 0.9520573779112154, 'bagging_fraction': 0.5794146114392404, 'min_child_samples': 19, 'min_child_weight': 45.69486149597815, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1439]\tvalid_0's binary_logloss: 0.488851\nroc-auc : 0.7420362043800001\n\n| \u001b[39m50       \u001b[39m | \u001b[39m0.742    \u001b[39m | \u001b[39m0.5794   \u001b[39m | \u001b[39m0.9521   \u001b[39m | \u001b[39m0.9831   \u001b[39m | \u001b[39m1.48     \u001b[39m | \u001b[39m19.41    \u001b[39m | \u001b[39m45.69    \u001b[39m | \u001b[39m72.08    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 99, 'lambda_l1': 0.3708608542107189, 'lambda_l2': 0.3513241809288954, 'feature_fraction': 0.7239341704938304, 'bagging_fraction': 0.6211620054812774, 'min_child_samples': 15, 'min_child_weight': 27.168994537885, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1390]\tvalid_0's binary_logloss: 0.488862\nroc-auc : 0.7420539392762128\n\n| \u001b[39m51       \u001b[39m | \u001b[39m0.7421   \u001b[39m | \u001b[39m0.6212   \u001b[39m | \u001b[39m0.7239   \u001b[39m | \u001b[39m0.3709   \u001b[39m | \u001b[39m0.3513   \u001b[39m | \u001b[39m14.79    \u001b[39m | \u001b[39m27.17    \u001b[39m | \u001b[39m99.28    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 83, 'lambda_l1': 0.6205163129389546, 'lambda_l2': 0.35016009859947395, 'feature_fraction': 0.7692052911170744, 'bagging_fraction': 0.6634910971391181, 'min_child_samples': 36, 'min_child_weight': 35.55666053872211, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1481]\tvalid_0's binary_logloss: 0.488797\nroc-auc : 0.7421126679812826\n\n| \u001b[39m52       \u001b[39m | \u001b[39m0.7421   \u001b[39m | \u001b[39m0.6635   \u001b[39m | \u001b[39m0.7692   \u001b[39m | \u001b[39m0.6205   \u001b[39m | \u001b[39m0.3502   \u001b[39m | \u001b[39m35.8     \u001b[39m | \u001b[39m35.56    \u001b[39m | \u001b[39m82.79    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 34, 'lambda_l1': 0.2985139274284838, 'lambda_l2': 0.13003903987517118, 'feature_fraction': 0.8733983335941248, 'bagging_fraction': 0.5752915423005136, 'min_child_samples': 12, 'min_child_weight': 40.092995146298804, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[2165]\tvalid_0's binary_logloss: 0.488701\nroc-auc : 0.7422698852795248\n\n| \u001b[39m53       \u001b[39m | \u001b[39m0.7423   \u001b[39m | \u001b[39m0.5753   \u001b[39m | \u001b[39m0.8734   \u001b[39m | \u001b[39m0.2985   \u001b[39m | \u001b[39m0.13     \u001b[39m | \u001b[39m11.53    \u001b[39m | \u001b[39m40.09    \u001b[39m | \u001b[39m34.42    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 85, 'lambda_l1': 0.09514848794235942, 'lambda_l2': 0.7553483010708104, 'feature_fraction': 0.8762045122631037, 'bagging_fraction': 0.574304946217167, 'min_child_samples': 8, 'min_child_weight': 28.50775402630973, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1549]\tvalid_0's binary_logloss: 0.488921\nroc-auc : 0.7418567883538256\n\n| \u001b[39m54       \u001b[39m | \u001b[39m0.7419   \u001b[39m | \u001b[39m0.5743   \u001b[39m | \u001b[39m0.8762   \u001b[39m | \u001b[39m0.09515  \u001b[39m | \u001b[39m0.7553   \u001b[39m | \u001b[39m8.213    \u001b[39m | \u001b[39m28.51    \u001b[39m | \u001b[39m85.48    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 33, 'lambda_l1': 0.7653118566179089, 'lambda_l2': 0.27255915106974005, 'feature_fraction': 0.9522092770807393, 'bagging_fraction': 0.5687823997959804, 'min_child_samples': 47, 'min_child_weight': 28.999088247336523, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nDid not meet early stopping. Best iteration is:\n[2393]\tvalid_0's binary_logloss: 0.488723\nroc-auc : 0.7421748595615563\n\n| \u001b[39m55       \u001b[39m | \u001b[39m0.7422   \u001b[39m | \u001b[39m0.5688   \u001b[39m | \u001b[39m0.9522   \u001b[39m | \u001b[39m0.7653   \u001b[39m | \u001b[39m0.2726   \u001b[39m | \u001b[39m47.48    \u001b[39m | \u001b[39m29.0     \u001b[39m | \u001b[39m32.53    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 37, 'lambda_l1': 0.8583353890488967, 'lambda_l2': 1.5718676206170679, 'feature_fraction': 0.7782721973771494, 'bagging_fraction': 0.7046793072652027, 'min_child_samples': 38, 'min_child_weight': 45.382708361231096, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nDid not meet early stopping. Best iteration is:\n[2327]\tvalid_0's binary_logloss: 0.488704\nroc-auc : 0.7422422987600124\n\n| \u001b[39m56       \u001b[39m | \u001b[39m0.7422   \u001b[39m | \u001b[39m0.7047   \u001b[39m | \u001b[39m0.7783   \u001b[39m | \u001b[39m0.8583   \u001b[39m | \u001b[39m1.572    \u001b[39m | \u001b[39m37.65    \u001b[39m | \u001b[39m45.38    \u001b[39m | \u001b[39m36.9     \u001b[39m |\n하이퍼파라미터: {'num_leaves': 47, 'lambda_l1': 0.910236222853373, 'lambda_l2': 0.5768302293911687, 'feature_fraction': 0.9356029096184415, 'bagging_fraction': 0.7438329712833381, 'min_child_samples': 18, 'min_child_weight': 42.688643739842234, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1951]\tvalid_0's binary_logloss: 0.488788\nroc-auc : 0.7420541544719923\n\n| \u001b[39m57       \u001b[39m | \u001b[39m0.7421   \u001b[39m | \u001b[39m0.7438   \u001b[39m | \u001b[39m0.9356   \u001b[39m | \u001b[39m0.9102   \u001b[39m | \u001b[39m0.5768   \u001b[39m | \u001b[39m18.27    \u001b[39m | \u001b[39m42.69    \u001b[39m | \u001b[39m47.48    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 95, 'lambda_l1': 0.38888128694570645, 'lambda_l2': 0.9115734121406336, 'feature_fraction': 0.9740711167829874, 'bagging_fraction': 0.5230341033418319, 'min_child_samples': 12, 'min_child_weight': 31.986940739825727, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1317]\tvalid_0's binary_logloss: 0.488928\nroc-auc : 0.7419091283107994\n\n| \u001b[39m58       \u001b[39m | \u001b[39m0.7419   \u001b[39m | \u001b[39m0.523    \u001b[39m | \u001b[39m0.9741   \u001b[39m | \u001b[39m0.3889   \u001b[39m | \u001b[39m0.9116   \u001b[39m | \u001b[39m12.06    \u001b[39m | \u001b[39m31.99    \u001b[39m | \u001b[39m94.84    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 78, 'lambda_l1': 0.42057298453623426, 'lambda_l2': 1.517042022140107, 'feature_fraction': 0.8414212116726962, 'bagging_fraction': 0.7816128214827316, 'min_child_samples': 35, 'min_child_weight': 28.28378181146632, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1469]\tvalid_0's binary_logloss: 0.488948\nroc-auc : 0.7418283973179813\n\n| \u001b[39m59       \u001b[39m | \u001b[39m0.7418   \u001b[39m | \u001b[39m0.7816   \u001b[39m | \u001b[39m0.8414   \u001b[39m | \u001b[39m0.4206   \u001b[39m | \u001b[39m1.517    \u001b[39m | \u001b[39m34.83    \u001b[39m | \u001b[39m28.28    \u001b[39m | \u001b[39m78.0     \u001b[39m |\n하이퍼파라미터: {'num_leaves': 37, 'lambda_l1': 0.43637684978016544, 'lambda_l2': 0.9411010553031602, 'feature_fraction': 0.7076644059055172, 'bagging_fraction': 0.5929179344405704, 'min_child_samples': 40, 'min_child_weight': 35.722214194242575, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nDid not meet early stopping. Best iteration is:\n[2494]\tvalid_0's binary_logloss: 0.488705\nroc-auc : 0.742258635870795\n\n| \u001b[39m60       \u001b[39m | \u001b[39m0.7423   \u001b[39m | \u001b[39m0.5929   \u001b[39m | \u001b[39m0.7077   \u001b[39m | \u001b[39m0.4364   \u001b[39m | \u001b[39m0.9411   \u001b[39m | \u001b[39m40.5     \u001b[39m | \u001b[39m35.72    \u001b[39m | \u001b[39m37.49    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 89, 'lambda_l1': 0.8917012714201686, 'lambda_l2': 0.8665402922611376, 'feature_fraction': 0.972215634872859, 'bagging_fraction': 0.6837096262122895, 'min_child_samples': 43, 'min_child_weight': 36.70121497138267, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1341]\tvalid_0's binary_logloss: 0.488952\nroc-auc : 0.7418326864265222\n\n| \u001b[39m61       \u001b[39m | \u001b[39m0.7418   \u001b[39m | \u001b[39m0.6837   \u001b[39m | \u001b[39m0.9722   \u001b[39m | \u001b[39m0.8917   \u001b[39m | \u001b[39m0.8665   \u001b[39m | \u001b[39m42.89    \u001b[39m | \u001b[39m36.7     \u001b[39m | \u001b[39m89.49    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 85, 'lambda_l1': 0.4405404398126288, 'lambda_l2': 0.5775788762355425, 'feature_fraction': 0.7204810621783189, 'bagging_fraction': 0.6294330262499519, 'min_child_samples': 46, 'min_child_weight': 35.58800584886126, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1395]\tvalid_0's binary_logloss: 0.488824\nroc-auc : 0.7421893151899732\n\n| \u001b[39m62       \u001b[39m | \u001b[39m0.7422   \u001b[39m | \u001b[39m0.6294   \u001b[39m | \u001b[39m0.7205   \u001b[39m | \u001b[39m0.4405   \u001b[39m | \u001b[39m0.5776   \u001b[39m | \u001b[39m45.68    \u001b[39m | \u001b[39m35.59    \u001b[39m | \u001b[39m85.07    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 76, 'lambda_l1': 0.6788795972671222, 'lambda_l2': 1.8137798964955956, 'feature_fraction': 0.9105562071202508, 'bagging_fraction': 0.505271564655831, 'min_child_samples': 28, 'min_child_weight': 27.46211583382531, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1287]\tvalid_0's binary_logloss: 0.488846\nroc-auc : 0.7421280041355496\n\n| \u001b[39m63       \u001b[39m | \u001b[39m0.7421   \u001b[39m | \u001b[39m0.5053   \u001b[39m | \u001b[39m0.9106   \u001b[39m | \u001b[39m0.6789   \u001b[39m | \u001b[39m1.814    \u001b[39m | \u001b[39m28.24    \u001b[39m | \u001b[39m27.46    \u001b[39m | \u001b[39m75.54    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 46, 'lambda_l1': 0.848601164737293, 'lambda_l2': 0.27398602292985164, 'feature_fraction': 0.7602427549558691, 'bagging_fraction': 0.7362678314625575, 'min_child_samples': 21, 'min_child_weight': 44.10347479381592, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1960]\tvalid_0's binary_logloss: 0.488745\nroc-auc : 0.7421999900852\n\n| \u001b[39m64       \u001b[39m | \u001b[39m0.7422   \u001b[39m | \u001b[39m0.7363   \u001b[39m | \u001b[39m0.7602   \u001b[39m | \u001b[39m0.8486   \u001b[39m | \u001b[39m0.274    \u001b[39m | \u001b[39m21.13    \u001b[39m | \u001b[39m44.1     \u001b[39m | \u001b[39m45.95    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 31, 'lambda_l1': 0.8742286490397031, 'lambda_l2': 1.9213708775138554, 'feature_fraction': 0.9930153443385332, 'bagging_fraction': 0.6033451989713122, 'min_child_samples': 45, 'min_child_weight': 27.479329470498726, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[2297]\tvalid_0's binary_logloss: 0.488696\nroc-auc : 0.7421439444174178\n\n| \u001b[39m65       \u001b[39m | \u001b[39m0.7421   \u001b[39m | \u001b[39m0.6033   \u001b[39m | \u001b[39m0.993    \u001b[39m | \u001b[39m0.8742   \u001b[39m | \u001b[39m1.921    \u001b[39m | \u001b[39m45.46    \u001b[39m | \u001b[39m27.48    \u001b[39m | \u001b[39m31.21    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 71, 'lambda_l1': 0.3594338587618463, 'lambda_l2': 1.413065016576255, 'feature_fraction': 0.9371293142620679, 'bagging_fraction': 0.5705701648093844, 'min_child_samples': 5, 'min_child_weight': 46.34998746650814, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1652]\tvalid_0's binary_logloss: 0.488854\nroc-auc : 0.7420149039463768\n\n| \u001b[39m66       \u001b[39m | \u001b[39m0.742    \u001b[39m | \u001b[39m0.5706   \u001b[39m | \u001b[39m0.9371   \u001b[39m | \u001b[39m0.3594   \u001b[39m | \u001b[39m1.413    \u001b[39m | \u001b[39m5.423    \u001b[39m | \u001b[39m46.35    \u001b[39m | \u001b[39m70.67    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 45, 'lambda_l1': 0.018265377712030295, 'lambda_l2': 1.830834668590727, 'feature_fraction': 0.9358795318016, 'bagging_fraction': 0.5764404545705497, 'min_child_samples': 27, 'min_child_weight': 49.8462930549505, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1872]\tvalid_0's binary_logloss: 0.488746\nroc-auc : 0.7422395249061574\n\n| \u001b[39m67       \u001b[39m | \u001b[39m0.7422   \u001b[39m | \u001b[39m0.5764   \u001b[39m | \u001b[39m0.9359   \u001b[39m | \u001b[39m0.01827  \u001b[39m | \u001b[39m1.831    \u001b[39m | \u001b[39m26.73    \u001b[39m | \u001b[39m49.85    \u001b[39m | \u001b[39m45.26    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 63, 'lambda_l1': 0.9530840415787656, 'lambda_l2': 0.49576830086692825, 'feature_fraction': 0.8766351578091269, 'bagging_fraction': 0.5743174889807824, 'min_child_samples': 42, 'min_child_weight': 33.208454469671246, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1598]\tvalid_0's binary_logloss: 0.488821\nroc-auc : 0.7421023385838669\n\n| \u001b[39m68       \u001b[39m | \u001b[39m0.7421   \u001b[39m | \u001b[39m0.5743   \u001b[39m | \u001b[39m0.8766   \u001b[39m | \u001b[39m0.9531   \u001b[39m | \u001b[39m0.4958   \u001b[39m | \u001b[39m41.92    \u001b[39m | \u001b[39m33.21    \u001b[39m | \u001b[39m62.56    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 55, 'lambda_l1': 0.9357670965595871, 'lambda_l2': 0.24813051975548905, 'feature_fraction': 0.8974234106526707, 'bagging_fraction': 0.7191530693330829, 'min_child_samples': 11, 'min_child_weight': 30.879922790605537, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1980]\tvalid_0's binary_logloss: 0.488851\nroc-auc : 0.7419354690639186\n\n| \u001b[39m69       \u001b[39m | \u001b[39m0.7419   \u001b[39m | \u001b[39m0.7192   \u001b[39m | \u001b[39m0.8974   \u001b[39m | \u001b[39m0.9358   \u001b[39m | \u001b[39m0.2481   \u001b[39m | \u001b[39m11.2     \u001b[39m | \u001b[39m30.88    \u001b[39m | \u001b[39m55.28    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 37, 'lambda_l1': 0.4239361222068718, 'lambda_l2': 0.8959230288440532, 'feature_fraction': 0.7886329752904959, 'bagging_fraction': 0.7025788485326744, 'min_child_samples': 37, 'min_child_weight': 27.060542451685276, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nDid not meet early stopping. Best iteration is:\n[2315]\tvalid_0's binary_logloss: 0.488768\nroc-auc : 0.7420942726906823\n\n| \u001b[39m70       \u001b[39m | \u001b[39m0.7421   \u001b[39m | \u001b[39m0.7026   \u001b[39m | \u001b[39m0.7886   \u001b[39m | \u001b[39m0.4239   \u001b[39m | \u001b[39m0.8959   \u001b[39m | \u001b[39m36.73    \u001b[39m | \u001b[39m27.06    \u001b[39m | \u001b[39m36.81    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 39, 'lambda_l1': 0.4405914740708746, 'lambda_l2': 1.6597852428190307, 'feature_fraction': 0.8622474988376232, 'bagging_fraction': 0.5298661661769396, 'min_child_samples': 33, 'min_child_weight': 34.83287270094506, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[2162]\tvalid_0's binary_logloss: 0.48871\nroc-auc : 0.7422125721283441\n\n| \u001b[39m71       \u001b[39m | \u001b[39m0.7422   \u001b[39m | \u001b[39m0.5299   \u001b[39m | \u001b[39m0.8622   \u001b[39m | \u001b[39m0.4406   \u001b[39m | \u001b[39m1.66     \u001b[39m | \u001b[39m32.68    \u001b[39m | \u001b[39m34.83    \u001b[39m | \u001b[39m38.61    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 42, 'lambda_l1': 0.8136568743021815, 'lambda_l2': 0.06655993965337448, 'feature_fraction': 0.7204982229423179, 'bagging_fraction': 0.7556176746619239, 'min_child_samples': 35, 'min_child_weight': 28.76846575056883, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[2137]\tvalid_0's binary_logloss: 0.488797\nroc-auc : 0.7420601651467692\n\n| \u001b[39m72       \u001b[39m | \u001b[39m0.7421   \u001b[39m | \u001b[39m0.7556   \u001b[39m | \u001b[39m0.7205   \u001b[39m | \u001b[39m0.8137   \u001b[39m | \u001b[39m0.06656  \u001b[39m | \u001b[39m35.31    \u001b[39m | \u001b[39m28.77    \u001b[39m | \u001b[39m41.56    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 61, 'lambda_l1': 0.8627668635772631, 'lambda_l2': 1.2346056964769208, 'feature_fraction': 0.8696952697400554, 'bagging_fraction': 0.5141739192161918, 'min_child_samples': 14, 'min_child_weight': 47.429646004253726, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1562]\tvalid_0's binary_logloss: 0.488821\nroc-auc : 0.742194905344557\n\n| \u001b[39m73       \u001b[39m | \u001b[39m0.7422   \u001b[39m | \u001b[39m0.5142   \u001b[39m | \u001b[39m0.8697   \u001b[39m | \u001b[39m0.8628   \u001b[39m | \u001b[39m1.235    \u001b[39m | \u001b[39m14.36    \u001b[39m | \u001b[39m47.43    \u001b[39m | \u001b[39m60.68    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 34, 'lambda_l1': 0.12779220526941126, 'lambda_l2': 1.7215192242460966, 'feature_fraction': 0.7135765843171423, 'bagging_fraction': 0.5403814844448807, 'min_child_samples': 16, 'min_child_weight': 36.82684706496041, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[2279]\tvalid_0's binary_logloss: 0.488776\nroc-auc : 0.7421109523378663\n\n| \u001b[39m74       \u001b[39m | \u001b[39m0.7421   \u001b[39m | \u001b[39m0.5404   \u001b[39m | \u001b[39m0.7136   \u001b[39m | \u001b[39m0.1278   \u001b[39m | \u001b[39m1.722    \u001b[39m | \u001b[39m16.17    \u001b[39m | \u001b[39m36.83    \u001b[39m | \u001b[39m34.26    \u001b[39m |\n하이퍼파라미터: {'num_leaves': 51, 'lambda_l1': 0.5254268482803882, 'lambda_l2': 1.9325817949477044, 'feature_fraction': 0.8283994514982524, 'bagging_fraction': 0.5929979039586151, 'min_child_samples': 30, 'min_child_weight': 35.59764691425934, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 52867, number of negative: 152213\n[LightGBM] [Info] Total Bins 718\n[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 64\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.257787 -> initscore=-1.057502\n[LightGBM] [Info] Start training from score -1.057502\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[2210]\tvalid_0's binary_logloss: 0.488721\nroc-auc : 0.7421954532053717\n\n| \u001b[39m75       \u001b[39m | \u001b[39m0.7422   \u001b[39m | \u001b[39m0.593    \u001b[39m | \u001b[39m0.8284   \u001b[39m | \u001b[39m0.5254   \u001b[39m | \u001b[39m1.933    \u001b[39m | \u001b[39m29.61    \u001b[39m | \u001b[39m35.6     \u001b[39m | \u001b[39m50.97    \u001b[39m |\n=============================================================================================================\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# 평가함수 점수가 최대일 때 하이퍼파라미터\nmax_params = optimizer.max['params']\nmax_params","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T16:36:28.646604Z","iopub.execute_input":"2025-02-18T16:36:28.646958Z","iopub.status.idle":"2025-02-18T16:36:28.654017Z","shell.execute_reply.started":"2025-02-18T16:36:28.646931Z","shell.execute_reply":"2025-02-18T16:36:28.653315Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"{'bagging_fraction': 0.5740665177925204,\n 'feature_fraction': 0.8681399435410332,\n 'lambda_l1': 0.27629142068769685,\n 'lambda_l2': 0.4712228966008998,\n 'min_child_samples': 39.69034039735031,\n 'min_child_weight': 49.54045683701298,\n 'num_leaves': 36.30620159152834}"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"# 정수형 하이퍼파라미터 변환\nmax_params['num_leaves'] = int(round(max_params['num_leaves']))\nmax_params['min_child_samples'] = int(round(max_params['min_child_samples']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T16:36:39.825568Z","iopub.execute_input":"2025-02-18T16:36:39.825886Z","iopub.status.idle":"2025-02-18T16:36:39.829644Z","shell.execute_reply.started":"2025-02-18T16:36:39.825860Z","shell.execute_reply":"2025-02-18T16:36:39.828964Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# 값이 고정된 하이퍼파라미터 추가\nmax_params.update(fixed_params)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T16:36:40.324578Z","iopub.execute_input":"2025-02-18T16:36:40.324881Z","iopub.status.idle":"2025-02-18T16:36:40.328278Z","shell.execute_reply.started":"2025-02-18T16:36:40.324856Z","shell.execute_reply":"2025-02-18T16:36:40.327401Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"max_params","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T16:36:41.533431Z","iopub.execute_input":"2025-02-18T16:36:41.533724Z","iopub.status.idle":"2025-02-18T16:36:41.539049Z","shell.execute_reply.started":"2025-02-18T16:36:41.533700Z","shell.execute_reply":"2025-02-18T16:36:41.538356Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"{'bagging_fraction': 0.5740665177925204,\n 'feature_fraction': 0.8681399435410332,\n 'lambda_l1': 0.27629142068769685,\n 'lambda_l2': 0.4712228966008998,\n 'min_child_samples': 40,\n 'min_child_weight': 49.54045683701298,\n 'num_leaves': 36,\n 'objective': 'binary',\n 'learning_rate': 0.005,\n 'bagging_freq': 1,\n 'force_row_wise': True,\n 'random_state': 1991}"},"metadata":{}}],"execution_count":20},{"cell_type":"markdown","source":"## 6. 모델 훈련","metadata":{}},{"cell_type":"code","source":"print(type(X))  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T16:47:55.305176Z","iopub.execute_input":"2025-02-18T16:47:55.305537Z","iopub.status.idle":"2025-02-18T16:47:55.309817Z","shell.execute_reply.started":"2025-02-18T16:47:55.305507Z","shell.execute_reply":"2025-02-18T16:47:55.309047Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"print(X.index)  # X의 인덱스 확인\nprint(train_idx[:10])  # train_idx 일부 확인\nprint(valid_idx[:10])  # valid_idx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T16:48:53.445701Z","iopub.execute_input":"2025-02-18T16:48:53.446014Z","iopub.status.idle":"2025-02-18T16:48:53.451764Z","shell.execute_reply.started":"2025-02-18T16:48:53.445991Z","shell.execute_reply":"2025-02-18T16:48:53.450676Z"}},"outputs":[{"name":"stdout","text":"RangeIndex(start=0, stop=256351, step=1)\n[ 0  1  2  3  4  6  7  8  9 10]\n[  5  12  20  41  63  86  87  98 113 133]\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\n\n# 층화 K 폴드 교차 검증기\nfolds = StratifiedKFold(n_splits=10, shuffle=True, random_state=1991)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T16:37:22.390887Z","iopub.execute_input":"2025-02-18T16:37:22.391191Z","iopub.status.idle":"2025-02-18T16:37:22.395084Z","shell.execute_reply.started":"2025-02-18T16:37:22.391165Z","shell.execute_reply":"2025-02-18T16:37:22.394164Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# OOF 방식으로 훈련된 모델로 검증 데이터 타깃값을 예측한 확률을 담을 1차원 배열\noof_val_preds = np.zeros(X.shape[0]) \n# OOF 방식으로 훈련된 모델로 테스트 데이터 타깃값을 예측한 확률을 담을 1차원 배열\noof_test_preds = np.zeros(X_test.shape[0]) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T16:37:24.232524Z","iopub.execute_input":"2025-02-18T16:37:24.232866Z","iopub.status.idle":"2025-02-18T16:37:24.236734Z","shell.execute_reply.started":"2025-02-18T16:37:24.232836Z","shell.execute_reply":"2025-02-18T16:37:24.236095Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# OOF 방식으로 모델 훈련, 검증, 예측\nfor idx, (train_idx, valid_idx) in enumerate(folds.split(X, y)):\n    # 각 폴드를 구분하는 문구 출력\n    print('#'*40, f'폴드 {idx+1} / 폴드 {folds.n_splits}', '#'*40)\n    print(type(X))  # DataFrame인지 확인\n    \n    # 훈련용 데이터, 검증용 데이터 설정\n    # X_train, y_train = X[train_idx], y[train_idx] # 훈련용 데이터\n    # X_valid, y_valid = X[valid_idx], y[valid_idx] # 검증용 데이터\n\n    # 데이터가 DataFrame인지 확인 후 인덱싱 방식 결정\n    if isinstance(X, pd.DataFrame):\n        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n    else:  # numpy.ndarray일 경우\n        X_train, X_valid = X[train_idx], X[valid_idx]\n\n    y_train, y_valid = y[train_idx], y[valid_idx]\n\n    # LightGBM 전용 데이터셋 생성\n    dtrain = lgb.Dataset(X_train, y_train) # LightGBM 전용 훈련 데이터셋\n    dvalid = lgb.Dataset(X_valid, y_valid) # LightGBM 전용 검증 데이터셋\n                          \n    # LightGBM 모델 훈련\n    lgb_model = lgb.train(params=max_params,    # 최적 하이퍼파라미터\n                          train_set=dtrain,     # 훈련 데이터셋\n                          num_boost_round=2500, # 부스팅 반복 횟수\n                          valid_sets=dvalid,    # 성능 평가용 검증 데이터셋\n                          feval = lgb_roc_auc,\n                          callbacks=[early_stopping(stopping_rounds=200)])\n    \n    # 테스트 데이터를 활용해 OOF 예측\n    oof_test_preds += lgb_model.predict(X_test)/folds.n_splits\n    # 모델 성능 평가를 위한 검증 데이터 타깃값 예측 \n    oof_val_preds[valid_idx] += lgb_model.predict(X_valid)\n    \n    # 검증 데이터 예측 확률에 대한 ROC-AUC\n    roc_auc = roc_auc_score(y_valid, oof_val_preds[valid_idx])\n    print(f'폴드 {idx+1} roc-auc : {roc_auc}\\n')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T17:02:34.162144Z","iopub.execute_input":"2025-02-18T17:02:34.162481Z","iopub.status.idle":"2025-02-18T17:12:23.946609Z","shell.execute_reply.started":"2025-02-18T17:02:34.162451Z","shell.execute_reply":"2025-02-18T17:12:23.945668Z"}},"outputs":[{"name":"stdout","text":"######################################## 폴드 1 / 폴드 10 ########################################\n<class 'pandas.core.frame.DataFrame'>\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Number of positive: 59605, number of negative: 171110\n[LightGBM] [Info] Total Bins 713\n[LightGBM] [Info] Number of data points in the train set: 230715, number of used features: 62\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258349 -> initscore=-1.054567\n[LightGBM] [Info] Start training from score -1.054567\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[2272]\tvalid_0's binary_logloss: 0.487502\tvalid_0's roc_auc: 0.740624\n폴드 1 roc-auc : 0.7406243710695207\n\n######################################## 폴드 2 / 폴드 10 ########################################\n<class 'pandas.core.frame.DataFrame'>\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Number of positive: 59606, number of negative: 171110\n[LightGBM] [Info] Total Bins 714\n[LightGBM] [Info] Number of data points in the train set: 230716, number of used features: 62\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054550\n[LightGBM] [Info] Start training from score -1.054550\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[2009]\tvalid_0's binary_logloss: 0.487008\tvalid_0's roc_auc: 0.740029\n폴드 2 roc-auc : 0.7400287151919758\n\n######################################## 폴드 3 / 폴드 10 ########################################\n<class 'pandas.core.frame.DataFrame'>\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Number of positive: 59606, number of negative: 171110\n[LightGBM] [Info] Total Bins 720\n[LightGBM] [Info] Number of data points in the train set: 230716, number of used features: 62\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258352 -> initscore=-1.054550\n[LightGBM] [Info] Start training from score -1.054550\nTraining until validation scores don't improve for 200 rounds\nDid not meet early stopping. Best iteration is:\n[2328]\tvalid_0's binary_logloss: 0.489677\tvalid_0's roc_auc: 0.735015\n폴드 3 roc-auc : 0.7350150971271894\n\n######################################## 폴드 4 / 폴드 10 ########################################\n<class 'pandas.core.frame.DataFrame'>\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Number of positive: 59605, number of negative: 171111\n[LightGBM] [Info] Total Bins 714\n[LightGBM] [Info] Number of data points in the train set: 230716, number of used features: 62\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n[LightGBM] [Info] Start training from score -1.054573\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1580]\tvalid_0's binary_logloss: 0.488369\tvalid_0's roc_auc: 0.739165\n폴드 4 roc-auc : 0.7391646665842204\n\n######################################## 폴드 5 / 폴드 10 ########################################\n<class 'pandas.core.frame.DataFrame'>\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Number of positive: 59605, number of negative: 171111\n[LightGBM] [Info] Total Bins 721\n[LightGBM] [Info] Number of data points in the train set: 230716, number of used features: 62\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n[LightGBM] [Info] Start training from score -1.054573\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1722]\tvalid_0's binary_logloss: 0.486963\tvalid_0's roc_auc: 0.741989\n폴드 5 roc-auc : 0.7419892651697146\n\n######################################## 폴드 6 / 폴드 10 ########################################\n<class 'pandas.core.frame.DataFrame'>\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Number of positive: 59605, number of negative: 171111\n[LightGBM] [Info] Total Bins 720\n[LightGBM] [Info] Number of data points in the train set: 230716, number of used features: 62\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n[LightGBM] [Info] Start training from score -1.054573\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1916]\tvalid_0's binary_logloss: 0.489257\tvalid_0's roc_auc: 0.736203\n폴드 6 roc-auc : 0.7362029850644803\n\n######################################## 폴드 7 / 폴드 10 ########################################\n<class 'pandas.core.frame.DataFrame'>\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Number of positive: 59605, number of negative: 171111\n[LightGBM] [Info] Total Bins 720\n[LightGBM] [Info] Number of data points in the train set: 230716, number of used features: 62\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n[LightGBM] [Info] Start training from score -1.054573\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1144]\tvalid_0's binary_logloss: 0.486487\tvalid_0's roc_auc: 0.744359\n폴드 7 roc-auc : 0.7443593322926223\n\n######################################## 폴드 8 / 폴드 10 ########################################\n<class 'pandas.core.frame.DataFrame'>\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Number of positive: 59605, number of negative: 171111\n[LightGBM] [Info] Total Bins 719\n[LightGBM] [Info] Number of data points in the train set: 230716, number of used features: 62\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n[LightGBM] [Info] Start training from score -1.054573\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1353]\tvalid_0's binary_logloss: 0.488191\tvalid_0's roc_auc: 0.740345\n폴드 8 roc-auc : 0.7403453381271565\n\n######################################## 폴드 9 / 폴드 10 ########################################\n<class 'pandas.core.frame.DataFrame'>\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Number of positive: 59605, number of negative: 171111\n[LightGBM] [Info] Total Bins 715\n[LightGBM] [Info] Number of data points in the train set: 230716, number of used features: 62\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n[LightGBM] [Info] Start training from score -1.054573\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1865]\tvalid_0's binary_logloss: 0.484627\tvalid_0's roc_auc: 0.745877\n폴드 9 roc-auc : 0.7458767985215851\n\n######################################## 폴드 10 / 폴드 10 ########################################\n<class 'pandas.core.frame.DataFrame'>\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Number of positive: 59605, number of negative: 171111\n[LightGBM] [Info] Total Bins 722\n[LightGBM] [Info] Number of data points in the train set: 230716, number of used features: 62\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258348 -> initscore=-1.054573\n[LightGBM] [Info] Start training from score -1.054573\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1685]\tvalid_0's binary_logloss: 0.487818\tvalid_0's roc_auc: 0.741425\n폴드 10 roc-auc : 0.7414250459169458\n\n","output_type":"stream"}],"execution_count":41},{"cell_type":"markdown","source":"## 7. 최종 결과","metadata":{}},{"cell_type":"code","source":"print('OOF 검증 데이터 roc-auc :', roc_auc_score(y, oof_val_preds))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T17:12:44.802263Z","iopub.execute_input":"2025-02-18T17:12:44.802605Z","iopub.status.idle":"2025-02-18T17:12:44.914520Z","shell.execute_reply.started":"2025-02-18T17:12:44.802575Z","shell.execute_reply":"2025-02-18T17:12:44.913585Z"}},"outputs":[{"name":"stdout","text":"OOF 검증 데이터 roc-auc : 0.7404601824298896\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"submission['probability'] = oof_test_preds\nsubmission.to_csv('baseline_ordinal_submission.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T17:13:24.019967Z","iopub.execute_input":"2025-02-18T17:13:24.020293Z","iopub.status.idle":"2025-02-18T17:13:24.209369Z","shell.execute_reply.started":"2025-02-18T17:13:24.020264Z","shell.execute_reply":"2025-02-18T17:13:24.208706Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}